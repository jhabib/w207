{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza\n",
    "#### Mohammad J. Habib - W207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "I picked Kaggle's Random Accts of Pizza (RAOP) classification task for my W207 final project. \n",
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "\n",
    "The goal is to classify Reddit posts in two categories: whether a post got a free pizza from a stranger (True) or not (False). About 75% of the posts in the RAOP dataset did not get a pizza. Randomly predicting \"no pizza\" or \"False\" for all classes would give an accuracy score of ~75% and a ROC AUC score of 0.50. \n",
    "\n",
    "I tried various sets of features and classifiers before I came up with this final model. You can read more about that in my final project presentation and review the experimentation in the notebooks on jhabib/w207/w207_final_project/background_work folder (github.com).\n",
    "\n",
    "To summarize, a bag of words approach performs really poorly on this task no matter the classifier used (ROC AUC ~0.52 at best). Ensembles, Neural Nets, Gaussian Mixture Models etc. all perform similarly poorly with bag of words. PCA on the bag of words did not help either.  It was not until I looked at numeric features in the data that the ROC AUC score started to show some improvement. It turned out that numeric features alone, without any meta features inferred from the data e.g. length of post, gave me the best result. I also found that sub-sampling the training dataset to get an equal ratio of pizza-getting (True) and no-pizza (False) posts helped improve the score.\n",
    "\n",
    "I used **xgboost's XGBClassifier** for the best result. You will need to install that on your machine before you can run this notebook. sklearn's GradientBoostingClassifier was not too far behind either but I left that be.\n",
    "\n",
    "My team consisted of me, myself and I.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data, and the features used in the model\n",
    "\n",
    "I used data available from the Standford website for this model. Link: https://cs.stanford.edu/~althoff/raop-dataset/\n",
    "\n",
    "I did not use data available from Kaggle for the submission because it does not provide labels with the test data. I did create a Kaggle submission for kicks which can be found in _kaggle_notebook.ipynb.\n",
    "\n",
    "### Features not used\n",
    "The Stanford dataset includes some columns (features) that literally tell you which post recevied a Pizza. I obviously avoided these features:\n",
    "\n",
    "+ giver_username_if_known (col. index 0)\n",
    "+ requester_received_pizza (23)\n",
    "+ requester_user_flair (29) - turns out requesters get flair when the get a pizza\n",
    "\n",
    "### Features used in this model \n",
    "+ number_of_downvotes_of_request_at_retrieval\n",
    "+ number_of_upvotes_of_request_at_retrieval\n",
    "+ post_was_edited\n",
    "+ request_number_of_comments_at_retrieval\n",
    "+ requester_account_age_in_days_at_request\n",
    "+ requester_account_age_in_days_at_retrieval\n",
    "+ requester_days_since_first_post_on_raop_at_request\n",
    "+ requester_days_since_first_post_on_raop_at_retrieval\n",
    "+ requester_number_of_comments_at_request\n",
    "+ requester_number_of_comments_at_retrieval\n",
    "+ requester_number_of_comments_in_raop_at_request\n",
    "+ requester_number_of_comments_in_raop_at_retrieval\n",
    "+ requester_number_of_posts_at_request\n",
    "+ requester_number_of_posts_at_retrieval\n",
    "+ requester_number_of_posts_on_raop_at_request\n",
    "+ requester_number_of_posts_on_raop_at_retrieval\n",
    "+ requester_number_of_subreddits_at_request\n",
    "+ requester_upvotes_minus_downvotes_at_request\n",
    "+ requester_upvotes_minus_downvotes_at_retrieval\n",
    "+ requester_upvotes_plus_downvotes_at_request\n",
    "+ requester_upvotes_plus_downvotes_at_retrieval\n",
    "\n",
    "OK, now that we have that out of the way, let's start by importing some packages and loading the data.\n",
    "\n",
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import urllib\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# download the data and extract the tarball \n",
    "# NOTE: change the url to http from https if you get a urllib error \n",
    "tf = urllib.URLopener()\n",
    "tf.retrieve(\"https://cs.stanford.edu/~althoff/raop-dataset/pizza_request_dataset.tar.gz\", \"pizza.tar.gz\")\n",
    "\n",
    "tar = tarfile.open(\"pizza.tar.gz\", \"r:gz\")\n",
    "for name in tar.getnames():\n",
    "    if name == \"pizza_request_dataset/pizza_request_dataset.json\":\n",
    "        member = tar.getmember(name)\n",
    "        f = tar.extractfile(member)\n",
    "        if f is not None:\n",
    "            json_data = f.read()\n",
    "\n",
    "# convert data to a pandas dataframe\n",
    "pizza_df = pd.read_json(json_data)\n",
    "feature_names = np.asarray([x for x in pizza_df[:0]])\n",
    "pizza_df = np.asarray(pizza_df)\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(pizza_df.shape[0]))\n",
    "pizza_df = pizza_df[shuffle]\n",
    "\n",
    "# keep only the features we need\n",
    "# and separate the labels\n",
    "features_to_keep = [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 28]\n",
    "X, y = pizza_df[:, features_to_keep], pizza_df[:, 23]\n",
    "\n",
    "# separate out train, dev and test data and labels\n",
    "# we've already shuffled this around before\n",
    "dev_data, dev_labels = X[:500], y[:500]\n",
    "test_data, test_labels = X[500:1000], y[500:1000]\n",
    "train_data, train_labels = X[1000:], y[1000:]\n",
    "\n",
    "\n",
    "# let's create a new training dataset with equal parts True and False posts\n",
    "train_pos = train_data[np.where(train_labels==1)]\n",
    "train_labels_pos = train_labels[np.where(train_labels==1)]\n",
    "train_neg = train_data[np.where(train_labels==0)]\n",
    "train_labels_neg = train_labels[np.where(train_labels==0)]\n",
    "\n",
    "train_neg_wanted = []\n",
    "train_labels_wanted = []\n",
    "\n",
    "for i in range(train_pos.shape[0]):\n",
    "    rand_index = random.randint(0, train_pos.shape[0])\n",
    "    train_neg_wanted.append(train_neg[int(rand_index)])\n",
    "    train_labels_wanted.append(train_labels_neg[int(rand_index)])\n",
    "\n",
    "train_neg_wanted = np.asarray(train_neg_wanted)\n",
    "train_labels_wanted = np.asarray(train_labels_wanted)\n",
    "\n",
    "train_new = np.concatenate((train_pos, train_neg_wanted), axis=0)\n",
    "train_labels_new = np.concatenate((train_labels_pos, train_labels_wanted), axis=0)\n",
    "\n",
    "# we need the labels in a binary format\n",
    "# Python can convert True, False to 1, 0\n",
    "train_labels_new = np.asarray(train_labels_new, dtype=int)\n",
    "dev_labels = np.asarray(dev_labels, dtype=int)\n",
    "test_labels = np.asarray(test_labels, dtype=int)\n",
    "\n",
    "# reshuffle train_new and train_labels_new\n",
    "shuffle = np.random.permutation(np.arange(train_new.shape[0]))\n",
    "train_new, train_labels_new = train_new[shuffle], train_labels_new[shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create helper functions\n",
    "\n",
    "Create some helper functions that will reduce typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def score_classifier(clf, train, train_labels, test, test_labels):\n",
    "    clf.fit(train, train_labels)\n",
    "    train_accuracy = metrics.accuracy_score(train_labels, clf.predict(train))\n",
    "    train_rocauc = metrics.roc_auc_score(train_labels, clf.predict(train))\n",
    "    test_accuracy = metrics.accuracy_score(test_labels, clf.predict(test))    \n",
    "    test_rocauc = metrics.roc_auc_score(test_labels, clf.predict(test))\n",
    "    print(\"Train Accuracy: %.4f, Train AUC: %.4f \\nTest Accuracy: %.4f, Test AUC: %.4f\\n\" \n",
    "          % (train_accuracy, train_rocauc, test_accuracy, test_rocauc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8640, Train AUC: 0.8640 \n",
      "Test Accuracy: 0.7700, Test AUC: 0.7763\n",
      "\n",
      "First baseline for the test data:\n",
      "Train Accuracy: 0.8640, Train AUC: 0.8640 \n",
      "Test Accuracy: 0.7780, Test AUC: 0.7857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's run a simple analysis at first\n",
    "gbm = xgb.XGBClassifier(objective='binary:logistic', seed=0)\n",
    "\n",
    "# Note that we are using dev_data here (not the held out test data)\n",
    "score_classifier(gbm, train_new, train_labels_new, dev_data, dev_labels)\n",
    "\n",
    "print(\"First baseline for the test data:\")\n",
    "score_classifier(gbm, train_new, train_labels_new, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not too shabby on the dev_data (the AUC is already ~0.28 points better than bag of words and coin toss). I think we can do a better job by tuning some parameters with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Best AUC: 0.9358\n",
      "Best Params: {'n_estimators': 250, 'learning_rate': 0.05, 'max_depth': 25}\n",
      "Train Accuracy: 1.0000, Train AUC: 1.0000 \n",
      "Test Accuracy: 0.7620, Test AUC: 0.7837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tuning round one\n",
    "params_ = {\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'learning_rate': [0.05, 0.07, 0.10],\n",
    "    'max_depth': np.arange(5, 35, 10), \n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier(objective='binary:logistic', seed=0)\n",
    "gsc = GridSearchCV(gbm, params_, cv=10, verbose=1, scoring='roc_auc', n_jobs=-1)\n",
    "gsc.fit(train_new, train_labels_new)\n",
    "\n",
    "print (\"Train Best AUC: %.4f\" % (gsc.best_score_))\n",
    "print (\"Best Params: %s\" % (gsc.best_params_))\n",
    "score_classifier(gsc.best_estimator_, train_new, train_labels_new, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too goot but may be we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  7.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Best AUC: 0.9346\n",
      "Best Params: {'min_child_weight': 2, 'gamma': 0.0}\n",
      "Train Accuracy: 1.0000, Train AUC: 1.0000 \n",
      "Test Accuracy: 0.7500, Test AUC: 0.7705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params_ = {\n",
    "    'min_child_weight':np.arange(2, 10, 4), \n",
    "    'gamma': np.arange(0, 1, 0.25), \n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier(n_estimators=500, learning_rate=0.07, max_depth=15, subsample=0.9, \n",
    "                        scale_pos_weight = 0.75, reg_alpha = 0.03, colsample_bytree=0.4, \n",
    "                        objective='binary:logistic', seed=0)\n",
    "\n",
    "gsc = GridSearchCV(gbm, params_, cv=10, verbose=1, scoring='roc_auc', n_jobs=-1)\n",
    "gsc.fit(train_new, train_labels_new)\n",
    "\n",
    "print (\"Train Best AUC: %.4f\" % (gsc.best_score_))\n",
    "print (\"Best Params: %s\" % (gsc.best_params_))\n",
    "score_classifier(gsc.best_estimator_, train_new, train_labels_new, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking xgboost\n",
    "\n",
    "Train Accuracy: 0.9480, Train AUC: 0.9480 \n",
    "Test Accuracy: 0.7920, Test AUC: 0.8090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def stack_clf(clf, train, train_labels, test):\n",
    "    \n",
    "    train_one, train_one_labels = train[train.shape[0]/2:,], train_labels[train_labels.shape[0]/2:,]\n",
    "    train_two, train_two_labels = train[:train.shape[0]/2,], train_labels[:train_labels.shape[0]/2,]\n",
    "    \n",
    "    clf_one = deepcopy(clf)\n",
    "    clf_one.fit(train_one, train_one_labels)\n",
    "    \n",
    "    preds_two = clf_one.predict_proba(train_two)\n",
    "    preds_test_one = clf_one.predict_proba(test_data)\n",
    "    \n",
    "    clf_two = deepcopy(clf)\n",
    "    clf_two.fit(train_two, train_two_labels)\n",
    "\n",
    "    preds_one = clf_one.predict_proba(train_one)\n",
    "    preds_test_two = clf_one.predict_proba(test_data)\n",
    "    \n",
    "    # meta_train\n",
    "    train_stack = np.concatenate((preds_one, preds_two), axis=0)\n",
    "    # meta_dev\n",
    "    test_stack = 0.5*(preds_test_one + preds_test_two)\n",
    "    \n",
    "    return np.column_stack((train, train_stack)), np.column_stack((test, test_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:   58.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Best AUC: 0.9304\n",
      "Best Params: {'min_child_weight': 2, 'gamma': 0.25}\n",
      "Train Accuracy: 1.0000, Train AUC: 1.0000 \n",
      "Test Accuracy: 0.7440, Test AUC: 0.7665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.XGBClassifier(n_estimators=500, learning_rate=0.07, max_depth=15, subsample=0.9, \n",
    "                        scale_pos_weight=0.75, reg_alpha=0.03, colsample_bytree=0.4, \n",
    "                        min_child_weight=2, gamma=0.5, \n",
    "                        objective='binary:logistic', seed=0)\n",
    "\n",
    "train_stack, dev_stack = stack_clf(gbm, train_new, train_labels_new, dev_data)\n",
    "\n",
    "params_ = {\n",
    "    'min_child_weight':np.arange(2, 10, 4), \n",
    "    'gamma': np.arange(0, 1, 0.25), \n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier(n_estimators=500, learning_rate=0.07, max_depth=15, subsample=0.9, \n",
    "                        scale_pos_weight = 0.75, reg_alpha = 0.03, colsample_bytree=0.4, \n",
    "                        objective='binary:logistic', seed=0)\n",
    "\n",
    "gsc = GridSearchCV(gbm, params_, cv=10, verbose=1, scoring='roc_auc', n_jobs=-1)\n",
    "gsc.fit(train_stack, train_labels_new)\n",
    "\n",
    "print (\"Train Best AUC: %.4f\" % (gsc.best_score_))\n",
    "print (\"Best Params: %s\" % (gsc.best_params_))\n",
    "score_classifier(gsc.best_estimator_, train_stack, train_labels_new, dev_stack, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OK, well that made things worse as before. Let's try one last thing.\n",
    "\n",
    "## BaggingClassifier with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\bagging.py:537: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9870, Train AUC: 0.9870 \n",
      "Test Accuracy: 0.7800, Test AUC: 0.8009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with the stacked data\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(base_estimator=xgb.XGBClassifier(n_estimators=500, learning_rate=0.07, max_depth=15, subsample=0.9, \n",
    "                        scale_pos_weight = 0.75, reg_alpha = 0.03, colsample_bytree=0.4, \n",
    "                        objective='binary:logistic', seed=0),\n",
    "                           n_estimators=10,\n",
    "                           bootstrap=True,\n",
    "                           oob_score=True,\n",
    "                           random_state=0)\n",
    "score_classifier(bc, train_stack, train_labels_new, dev_stack, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9887, Train AUC: 0.9887 \n",
      "Test Accuracy: 0.7700, Test AUC: 0.7865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without the stacked data\n",
    "bc = BaggingClassifier(base_estimator=xgb.XGBClassifier(n_estimators=500, learning_rate=0.07, max_depth=15, subsample=0.9, \n",
    "                        scale_pos_weight = 0.75, reg_alpha = 0.03, colsample_bytree=0.4, \n",
    "                        objective='binary:logistic', seed=0),\n",
    "                           n_estimators=10,\n",
    "                           bootstrap=True,\n",
    "                           oob_score=True,\n",
    "                           random_state=0)\n",
    "score_classifier(bc, train_new, train_labels_new, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for the test data\n",
    "\n",
    "We can now get a score for the test data that we've held out. Let's quickly rehash the plan:\n",
    "    - Stack the training and test data\n",
    "    - Use BaggingClassifier with XGBClassifier\n",
    "    \n",
    "Let's get to it then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9887, Train AUC: 0.9887 \n",
      "Test Accuracy: 0.7660, Test AUC: 0.7901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bc = BaggingClassifier(base_estimator=xgb.XGBClassifier(n_estimators=500, learning_rate=0.07, max_depth=15, subsample=0.9, \n",
    "                        scale_pos_weight = 0.75, reg_alpha = 0.03, colsample_bytree=0.4, \n",
    "                        objective='binary:logistic', seed=0),\n",
    "                           n_estimators=10,\n",
    "                           bootstrap=True,\n",
    "                           oob_score=True,\n",
    "                           random_state=0)\n",
    "train_stack, test_stack = stack_clf(bc, train_new, train_labels_new, test_data)\n",
    "\n",
    "score_classifier(bc, train_stack, train_labels_new, test_stack, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose the score of ~0.79 is not too bad on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use Kaggle's Random Accts of Pizza classification task for my project.\n",
    "\n",
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "\n",
    "Let's start by importing some libraries and loading the data into an ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [giver_username_if_known, in_test_set, number_of_downvotes_of_request_at_retrieval, number_of_upvotes_of_request_at_retrieval, post_was_edited, request_id, request_number_of_comments_at_retrieval, request_text, request_text_edit_aware, request_title, requester_account_age_in_days_at_request, requester_account_age_in_days_at_retrieval, requester_days_since_first_post_on_raop_at_request, requester_days_since_first_post_on_raop_at_retrieval, requester_number_of_comments_at_request, requester_number_of_comments_at_retrieval, requester_number_of_comments_in_raop_at_request, requester_number_of_comments_in_raop_at_retrieval, requester_number_of_posts_at_request, requester_number_of_posts_at_retrieval, requester_number_of_posts_on_raop_at_request, requester_number_of_posts_on_raop_at_retrieval, requester_number_of_subreddits_at_request, requester_received_pizza, requester_subreddits_at_request, requester_upvotes_minus_downvotes_at_request, requester_upvotes_minus_downvotes_at_retrieval, requester_upvotes_plus_downvotes_at_request, requester_upvotes_plus_downvotes_at_retrieval, requester_user_flair, requester_username, unix_timestamp_of_request, unix_timestamp_of_request_utc]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import urllib\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# download the data and extract the tarball \n",
    "tf = urllib.URLopener()\n",
    "\n",
    "# change the url to http from https if you get a urllib error\n",
    "tf.retrieve(\"https://cs.stanford.edu/~althoff/raop-dataset/pizza_request_dataset.tar.gz\", \"pizza.tar.gz\")\n",
    "\n",
    "tar = tarfile.open(\"pizza.tar.gz\", \"r:gz\")\n",
    "for name in tar.getnames():\n",
    "    if name == \"pizza_request_dataset/pizza_request_dataset.json\":\n",
    "        member = tar.getmember(name)\n",
    "        f = tar.extractfile(member)\n",
    "        if f is not None:\n",
    "            json_data = f.read()\n",
    "\n",
    "# convert data to a pandas dataframe\n",
    "pizza_df = pd.read_json(json_data)\n",
    "print(pizza_df[:0])\n",
    "pizza_df = np.asarray(pizza_df)\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(pizza_df.shape[0]))\n",
    "pizza_df = pizza_df[shuffle]\n",
    "\n",
    "# extract test and train data and labels\n",
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "test_data, test_labels = np.delete(pizza_df[500:1000], 23, axis=1), [x for x in pizza_df[500:1000, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a baseline\n",
    "\n",
    "Let's establish a baseline using BernoulliNB, MultinomialNB and LogisticRegression. We will use just the post text (corresponding to column 7 in pizza_df) and the \"requester_received_pizza\" outcome (True, False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define a helper function to perform the analysis \n",
    "def perform_analysis(train_data, train_labels, dev_data, dev_features, \n",
    "                     vectorizer=CountVectorizer(), clf = BernoulliNB(), \n",
    "                     gsc_params = {}):\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(train_data)\n",
    "    dev_data_features = vectorizer.transform(dev_data)\n",
    "    \n",
    "    print(\"RESULTS FOR Default %s : -------------------------------\" % (clf))\n",
    "    clf.fit(train_data_features, train_labels)\n",
    "    print(\"f1_score: %s\\naccuracy_score: %s\\nroc_auc_score: %s\\n\" \n",
    "              % (metrics.f1_score(dev_labels, clf.predict(dev_data_features)), \n",
    "          metrics.accuracy_score(dev_labels, clf.predict(dev_data_features)), \n",
    "                metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))\n",
    "    \n",
    "    print(\"Calculating Cross Vaidation Scores: \")\n",
    "    scores = cross_validation.cross_val_score(clf, train_data_features, train_labels, cv=10, scoring='f1_weighted')\n",
    "    print(\"Scores: %s\\n\" % (scores))\n",
    "    \n",
    "    # Search for the best estimator\n",
    "    print(\"STARTING GRID SEARCH...\")\n",
    "    gsc = GridSearchCV(clf, gsc_params, n_jobs=-1)\n",
    "    gsc.fit(train_data_features, train_labels)\n",
    "    print(\"Best estimator: %s\\nBest alpha: %s\\nBest score: %s\\nScorer function: %s\\n\" \n",
    "          % (gsc.best_estimator_, gsc.best_params_, gsc.best_score_, gsc.scorer_))\n",
    "    # return gsc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB using CountVectorizer\n",
      "RESULTS FOR Default BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.305555555556\n",
      "accuracy_score: 0.7\n",
      "roc_auc_score: 0.554981518817\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.68923924  0.69364719  0.68049658  0.67924009  0.69507729  0.69611298\n",
      "  0.67960775  0.66109026  0.68012268  0.67949566]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: BernoulliNB(alpha=0.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.0}\n",
      "Best score: 0.737315350032\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "BernoulliNB using TfidfVectorizer\n",
      "RESULTS FOR Default BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.212290502793\n",
      "accuracy_score: 0.718\n",
      "roc_auc_score: 0.531207997312\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.6948353   0.69313005  0.70509014  0.69313005  0.70155229  0.67149621\n",
      "  0.67980691  0.65757883  0.68676729  0.67902348]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: BernoulliNB(alpha=0.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.0}\n",
      "Best score: 0.736887176194\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB using CountVectorizer\n",
    "print(\"BernoulliNB using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = BernoulliNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})\n",
    "\n",
    "# BernoulliNB using TfidfVectorizer\n",
    "print(\"BernoulliNB using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'), clf = BernoulliNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB using CountVectorizer\n",
      "RESULTS FOR Default MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.0915032679739\n",
      "accuracy_score: 0.722\n",
      "roc_auc_score: 0.503150201613\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.69658503  0.67541664  0.67977306  0.67645383  0.69491832  0.6602577\n",
      "  0.6839763   0.63693178  0.64502397  0.6718563 ]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: MultinomialNB(alpha=0.98999999999999999, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.98999999999999999}\n",
      "Best score: 0.731749090131\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "MultinomialNB using TfidfVectorizer\n",
      "RESULTS FOR Default MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.0\n",
      "accuracy_score: 0.744\n",
      "roc_auc_score: 0.5\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.64573692  0.64468864  0.64573692  0.64573692  0.64790979  0.64790979\n",
      "  0.64790979  0.64719664  0.64719664  0.64719664]\n",
      "\n",
      "STARTING GRID SEARCH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: MultinomialNB(alpha=0.91000000000000003, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.91000000000000003}\n",
      "Best score: 0.75294369514\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB using CountVectorizer\n",
    "print(\"MultinomialNB using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = MultinomialNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})\n",
    "\n",
    "# MultinomialNB using TfidfVectorizer\n",
    "print(\"MultinomialNB using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'), clf = MultinomialNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression using CountVectorizer\n",
      "RESULTS FOR Default LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) : -------------------------------\n",
      "f1_score: 0.209523809524\n",
      "accuracy_score: 0.668\n",
      "roc_auc_score: 0.50529233871\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.71056113  0.70374342  0.68475426  0.70022931  0.66436359  0.68405106\n",
      "  0.69450078  0.657613    0.68159917  0.70261076]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: LogisticRegression(C=0.02, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best alpha: {'C': 0.02}\n",
      "Best score: 0.754656390495\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "LogisticRegression using TfidfVectorizer\n",
      "RESULTS FOR Default LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) : -------------------------------\n",
      "f1_score: 0.0296296296296\n",
      "accuracy_score: 0.738\n",
      "roc_auc_score: 0.501092069892\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.66988649  0.67487664  0.65482947  0.67362827  0.66645446  0.66645446\n",
      "  0.66874512  0.65741875  0.67469176  0.66339267]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: LogisticRegression(C=0.76000000000000001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best alpha: {'C': 0.76000000000000001}\n",
      "Best score: 0.75594091201\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression using CountVectorizer\n",
    "print(\"LogisticRegression using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = LogisticRegression(), \n",
    "                 gsc_params = {'C': np.arange(0.01, 1, 0.01)})\n",
    "\n",
    "# LogisticRegression using TfidfVectorizer\n",
    "print(\"LogisticRegression using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(), clf = LogisticRegression(), \n",
    "                 gsc_params = {'C': np.arange(0.01, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier using CountVectorizer\n",
      "RESULTS FOR Default KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') : -------------------------------\n",
      "f1_score: 0.103896103896\n",
      "accuracy_score: 0.724\n",
      "roc_auc_score: 0.507056451613\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.66880342  0.65150464  0.67415834  0.66070829  0.65334379  0.66829823\n",
      "  0.64546451  0.65698709  0.66896721  0.64462871]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=52, p=2,\n",
      "           weights='uniform')\n",
      "Best alpha: {'n_neighbors': 52}\n",
      "Best score: 0.75315778206\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "KNeighborsClassifier using TfidfVectorizer\n",
      "RESULTS FOR Default KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') : -------------------------------\n",
      "f1_score: 0.0285714285714\n",
      "accuracy_score: 0.728\n",
      "roc_auc_score: 0.494371639785\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.65019014  0.66067982  0.65678407  0.64669785  0.6409107   0.64967935\n",
      "  0.63943053  0.64476271  0.64362025  0.64296913]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "           weights='uniform')\n",
      "Best alpha: {'n_neighbors': 12}\n",
      "Best score: 0.75294369514\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier using CountVectorizer\n",
    "print(\"KNeighborsClassifier using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = KNeighborsClassifier(), \n",
    "                 gsc_params = {'n_neighbors': np.arange(1, 100, 1)})\n",
    "\n",
    "# KNeighborsClassifier using TfidfVectorizer\n",
    "print(\"KNeighborsClassifier using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(), clf = KNeighborsClassifier(), \n",
    "                 gsc_params = {'n_neighbors': np.arange(1, 100, 1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, using default options with KNearestNeighbors, BernoulliNB, MultinomialNB and LogisticRegression yielded accuracy better that random guessing. All three classifiers scored between approximately 70% to 75%. We can use this as a baseline and look to improve the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better baseline?\n",
    "\n",
    "I suppose 75% accuracy is a good baseline to start with. Let's look to improve this in the next steps. I will try a pipeline as the first attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline (chaining classifiers)\n",
    "\n",
    "Let's see if chained transformation via CountVectorizer and TfiddfTransformer, followed by LogisticRegression, improves the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best alpha: {'vect__ngram_range': (1, 1), 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "Best score: 0.756797259687\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# let's learn about pipelines; they will come in handy for ensembles\n",
    "# ****code below is taken from scikit-learn's documentation on pipelines****\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "# **** end of code taken from scikit-learn's documentation ****\n",
    "\n",
    "gsc = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "gsc.fit(train_data[:, 7], train_labels)\n",
    "print(\"Best estimator: %s\\nBest alpha: %s\\nBest score: %s\\nScorer function: %s\\n\" \n",
    "      % (gsc.best_estimator_, gsc.best_params_, gsc.best_score_, gsc.scorer_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Randomized trees and Ensembles\n",
    "Let's first look at the results of randomized trees and then use an ensemble (the VotingClassifier). Let's also convert user's subreddits to a \"bag of words\" and run that through randomized trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier score: 0.744\n",
      "ExtraTreesClassifier roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000, analyzer='word', norm='l2', stop_words='english')\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf.fit(train_data_features, train_labels)\n",
    "\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(dev_data_features, dev_labels)))\n",
    "\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier score: 0.722\n",
      "ExtraTreesClassifier roc_auc_score: 0.485215053763\n",
      "ExtraTreesClassifier score: 0.955469920788\n",
      "ExtraTreesClassifier roc_auc_score: 0.910169791786\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees\n",
    "# With subreddit membership as a bag of words\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000, analyzer='word', norm='l2', stop_words='english')\n",
    "\n",
    "td = [' '.join(x) for x in train_data[:, 23]]\n",
    "dd = [' '.join(x) for x in dev_data[:, 23]]\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(td)\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "dev_data_features = vectorizer.transform(dd)\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf.fit(train_data_features, train_labels)\n",
    "\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(dev_data_features, dev_labels)))\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(train_data_features, train_labels)))\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, clf.predict(train_data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.748\n",
      "VotingClassifier roc_auc_score: 0.512936827957\n",
      "VotingClassifier accuracy score: 0.868764718476\n",
      "VotingClassifier roc_auc_score: 0.734402079723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = BernoulliNB()\n",
    "clf3 = LogisticRegression()\n",
    "clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "vclf1 = VotingClassifier(estimators=[('knn', clf1), ('bnb', clf2), ('lr', clf3), ('etc', clf4)], voting='hard')\n",
    "\n",
    "vclf1.fit(train_data_features, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf1.predict(dev_data_features))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf1.predict(dev_data_features))))\n",
    "\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(train_labels, vclf1.predict(train_data_features))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, vclf1.predict(train_data_features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm surprised to see that the ExtraTreesClassifier returns 72.2% accuracy for the subreddit membership which is marginally lower than the accuracy from the post itself (74%).\n",
    "However, the overall accuracy is similar to the results of individual classifiers used earlier. It appears that a bag of words approach alone is not going to cut it. \n",
    "\n",
    "Let's look to improve upon this by first using a FeatureUnion on post words and subreddits (bag of words, yet again). After that I will look to expand the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Unions to improve the baseline\n",
    "\n",
    "\n",
    "... to be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline accuracy score: 0.746\n",
      "Pipeline roc_auc_score: 0.50390625\n",
      "Pipeline accuracy score: 1.0\n",
      "Pipeline roc_auc_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ItemSelector class COPIED VERBATIM from:\n",
    "# http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "post_train_data = train_data[:, 7]\n",
    "post_dev_data = dev_data[:, 7] \n",
    "\n",
    "title_train_data = train_data[:, 9]\n",
    "title_dev_data = dev_data[:, 9] \n",
    "\n",
    "\n",
    "subreddits_train_data = [' '.join(x) for x in train_data[:, 23]] \n",
    "subreddits_dev_data = [' '.join(x) for x in dev_data[:, 23]]\n",
    "\n",
    "pipeline_train_data = {'title': title_train_data, \n",
    "                       'posts': post_train_data, \n",
    "                       'subreddits': subreddits_train_data}\n",
    "\n",
    "pipeline_dev_data = {'title': title_dev_data, \n",
    "                     'posts': post_dev_data, \n",
    "                     'subreddits': subreddits_dev_data}\n",
    "pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                    ('title', Pipeline([\n",
    "                                ('selector', ItemSelector(key='title')), \n",
    "                                ('cv', CountVectorizer()), \n",
    "                            ])),\n",
    "                    \n",
    "                    ('subreddits', Pipeline([\n",
    "                                ('selector', ItemSelector(key='subreddits')), \n",
    "                                ('cv', CountVectorizer()), \n",
    "                            ])),\n",
    "                    ('post', Pipeline([\n",
    "                                ('selector', ItemSelector(key='posts')), \n",
    "                                ('vect', TfidfVectorizer(min_df=0.01, stop_words='english')), \n",
    "                                ('best', TruncatedSVD(n_components=50)), \n",
    "                            ])), \n",
    "                    \n",
    "                ], \n",
    "            transformer_weights={\n",
    "                    'title': 0.5,\n",
    "                    'subreddits': 0.4, \n",
    "                    'post': 0.7, \n",
    "                }, \n",
    "            )), \n",
    "        ('clf', ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy'))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(pipeline_train_data, train_labels)\n",
    "\n",
    "print(\"Pipeline accuracy score: %s\" % (metrics.accuracy_score(dev_labels, pipeline.predict(pipeline_dev_data))))\n",
    "print(\"Pipeline roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, pipeline.predict(pipeline_dev_data))))\n",
    "\n",
    "print(\"Pipeline accuracy score: %s\" % (metrics.accuracy_score(train_labels, pipeline.predict(pipeline_train_data))))\n",
    "print(\"Pipeline roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, pipeline.predict(pipeline_train_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now ain't that a b\\*\\*\\*\\*\\!\n",
    "\n",
    "It looks like a bag-of-words approach is not going to cut it. But all is not lost yet. We can start adding some other features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paying it forward, reciprocity, give & take\n",
    "\n",
    "Let's evaluate notions of give and take and reciprocity in the posts. May be the potential for getting something in return affects people's decision to buy some random person a pizza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. of NOT getting pizza: 0.75\n",
      "Prob. of NOT getting pizza given reciprocity: 0.20 \n",
      "\n",
      "Prob. of getting pizza: 0.25\n",
      "Prob. of getting pizza given reciprocity: 0.26 \n",
      "\n",
      "Prob. of reciprocity in all posts: 0.22 \n",
      "\n",
      "Posterior prob. of getting pizza given reciprocity: 0.29\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "reciprocity_terms = ['pay it forward', 'paying it forward', 'forward',  \n",
    "                     'reciprocate', 'give back', 'trade', 'pay it back'\n",
    "                    'pic', 'pics', 'story', 'stories']\n",
    "\n",
    "def is_phrase_found(phrase_list, target_text):\n",
    "    found = False\n",
    "    for t in reciprocity_terms:\n",
    "        if t in target_text:\n",
    "            found = True\n",
    "            break\n",
    "    return found\n",
    "\n",
    "def get_phrase_freq(phrase_list, target_text):\n",
    "    found = 0\n",
    "    for t in reciprocity_terms:\n",
    "        for d in target_text:\n",
    "            if t in d:\n",
    "                found += 1\n",
    "    return found\n",
    "\n",
    "def calc_odds(data, query_terms):\n",
    "    f = 0.\n",
    "    for d in data:\n",
    "        if(is_phrase_found(query_terms, d)):\n",
    "            f += 1\n",
    "    return f / len(data)\n",
    "\n",
    "train_gotpizza = pizza_df[1000:][pizza_df[1000:][:, 23] == True][:, 7]\n",
    "train_nopizza = pizza_df[1000:][pizza_df[1000:][:, 23] == False][:, 7]\n",
    "dev_gotpizza = pizza_df[:500][pizza_df[:500][:, 23] == True][:, 7]\n",
    "dev_nopizza = pizza_df[:500][pizza_df[:500][:, 23] == False][:, 7]\n",
    "\n",
    "print(\"Prob. of NOT getting pizza: %.2f\" % (len(train_nopizza) / float(len(train_data))))\n",
    "p1 = calc_odds(train_nopizza, reciprocity_terms)\n",
    "print(\"Prob. of NOT getting pizza given reciprocity: %.2f \\n\" % p1)\n",
    "\n",
    "p2 = len(train_gotpizza) / float(len(train_data))\n",
    "print(\"Prob. of getting pizza: %.2f\" % p2)\n",
    "p3 = calc_odds(train_gotpizza, reciprocity_terms)\n",
    "print(\"Prob. of getting pizza given reciprocity: %.2f \\n\" % p3)\n",
    "\n",
    "p4 = calc_odds(train_data[:, 7], reciprocity_terms)\n",
    "print(\"Prob. of reciprocity in all posts: %.2f \\n\" % p4)\n",
    "\n",
    "# Calculate the posterior odds of the desired Event (E), that is getting a pizza\n",
    "# P(E | x) = p(x | E) * p(E) / p(x) \n",
    "print(\"Posterior prob. of getting pizza given reciprocity: %.2f\" % ( p3 * p2 / p4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that reciprocity has a bit of an effect on the likelihood of a post getting a pizza, let's add that as a feature to our pipeline. So may be we can hand adjust the probability of getting a pizza for posts that contain a notion of reciprocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior accuracy score: 0.746\n",
      "Posterior accuracy score: 0.746\n",
      "Posterior roc_auc_score score: 0.506468413978\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "\n",
    "et.fit(train_data_features, train_labels)\n",
    "print(\"Prior accuracy score: %s\" % metrics.accuracy_score(dev_labels, et.predict(dev_data_features)))\n",
    "\n",
    "dev_probs = et.predict_proba(dev_data_features)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    if(is_phrase_found(reciprocity_terms, t)):\n",
    "        dev_probs[i][1] = dev_probs[i][1] * 1.16\n",
    "# write our own prediction\n",
    "dev_predicted_labels = []\n",
    "for i, (f, t) in enumerate(dev_probs):\n",
    "    if(t > f): \n",
    "        dev_predicted_labels.append(True)\n",
    "    else:\n",
    "        dev_predicted_labels.append(False)\n",
    "\n",
    "print(\"Posterior accuracy score: %s\" % metrics.accuracy_score(dev_labels, dev_predicted_labels))\n",
    "print(\"Posterior roc_auc_score score: %s\" % metrics.roc_auc_score(dev_labels, dev_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the simplistic approach of adjusting the probabilities did not work. We can still use this result though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Numeric Features\n",
    "\n",
    "Since the bag of words approach alone did not fare much better than random guessing, at least in the roc_auc_score, let's look at some numeric features. As well as looking at features available directly in the data, we will also calcualte some numeric features: binary code reciprocity and calculate post length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binary code reciprocity\n",
    "reciproc_train_data = np.zeros(len(train_data[:, 7]))\n",
    "reciproc_train_data.reshape(len(train_data[:, 7]), 1)\n",
    "for i, t in enumerate(train_data[:, 7]):\n",
    "    reciproc_train_data[i] = int(is_phrase_found(reciprocity_terms, t))\n",
    "\n",
    "reciproc_dev_data = np.zeros(len(dev_data[:, 7]))\n",
    "reciproc_dev_data.reshape(len(dev_data[:, 7]), 1)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    reciproc_dev_data[i] = int(is_phrase_found(reciprocity_terms, t)) \n",
    "\n",
    "# Calculate Post Length\n",
    "length_train_post = np.zeros(len(train_data[:, 7]))\n",
    "length_train_post.reshape(len(train_data[:, 7]), 1)\n",
    "for i, t in enumerate(train_data[:, 7]):\n",
    "    length_train_post[i] = len(t.split())\n",
    "\n",
    "length_dev_post = np.zeros(len(dev_data[:, 7]))\n",
    "length_dev_post.reshape(len(dev_data[:, 7]), 1)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    length_dev_post[i] = len(t.split())\n",
    "\n",
    "# selected numeric columns from train_data and dev_data\n",
    "# number represents the column index starting at 0\n",
    "# 2 \t number_of_downvotes_of_request_at_retrieval\n",
    "# 6 \t request_number_of_comments_at_retrieval\n",
    "# 10\t requester_account_age_in_days_at_request\n",
    "# 12\t requester_days_since_first_post_on_raop_at_request\n",
    "# 14\t requester_number_of_comments_at_request\n",
    "# 16\t requester_number_of_comments_in_raop_at_request\n",
    "# 18\t requester_number_of_posts_at_request\n",
    "# 20\t requester_number_of_posts_on_raop_at_request\n",
    "# 22\t requester_number_of_subreddits_at_request\n",
    "\n",
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine the numeric data \n",
    "train_temp = []\n",
    "for i in range(len(train_data[:, 7])):\n",
    "    train_temp.append(np.hstack((train_numeric_data[0], reciproc_train_data[0])))\n",
    "train_combined = []\n",
    "for i in range(len(train_data[:, 7])):\n",
    "    train_combined.append(np.hstack((train_temp[0], length_train_post[0])))\n",
    "\n",
    "dev_temp = []\n",
    "for i in range(len(dev_data[:, 7])):\n",
    "    dev_temp.append(np.hstack((dev_numeric_data[0], reciproc_dev_data[0])))\n",
    "dev_combined = []\n",
    "for i in range(len(dev_data[:, 7])):\n",
    "    dev_combined.append(np.hstack((dev_temp[0], length_dev_post[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n",
      "0.58639952957\n",
      "0.769428387925\n",
      "0.575281906304\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, lr.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, lr.predict(dev_numeric_data)))\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, lr.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, lr.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.579175067204\n",
      "0.97580817812\n",
      "0.95453316868\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier()\n",
    "et.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, et.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, et.predict(dev_numeric_data)))\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, et.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, et.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n",
      "0.622521841398\n",
      "0.819738813958\n",
      "0.693112655589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(dev_numeric_data))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_text_features = vectorizer.fit_transform(train_data[:, 7]).todense()\n",
    "dev_text_features = vectorizer.transform(dev_data[:, 7]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_features = np.concatenate((train_text_features, train_numeric_data), axis=1)\n",
    "all_dev_features = np.concatenate((dev_text_features, dev_numeric_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754\n",
      "0.598958333333\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(all_train_features, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(all_dev_features)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(all_dev_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "atf_normalized = preprocessing.normalize(all_train_features, norm='l2')\n",
    "adf_normalized = preprocessing.normalize(all_dev_features, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724\n",
      "0.496807795699\n"
     ]
    }
   ],
   "source": [
    "gb = ExtraTreesClassifier()\n",
    "gb.fit(atf_normalized, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(adf_normalized)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(adf_normalized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.744\n",
      "VotingClassifier roc_auc_score: 0.505124327957\n"
     ]
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = BernoulliNB()\n",
    "clf3 = LogisticRegression()\n",
    "clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "vclf2 = VotingClassifier(estimators=[('knn', clf1), ('bnb', clf2), ('lr', clf3), ('etc', clf4)], voting='hard')\n",
    "\n",
    "\n",
    "vclf2.fit(atf_normalized, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf2.predict(adf_normalized))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf2.predict(adf_normalized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.746\n",
      "VotingClassifier roc_auc_score: 0.509030577957\n"
     ]
    }
   ],
   "source": [
    "clf5 = GradientBoostingClassifier()\n",
    "vclf3 = VotingClassifier(estimators=[('lr', clf3), ('etc', clf4), ('gb', clf5)], voting='hard')\n",
    "\n",
    "\n",
    "vclf3.fit(atf_normalized, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf3.predict(adf_normalized))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf3.predict(adf_normalized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_PCA(training_data, K=np.arange(1, 10, 1)):\n",
    "### STUDENT START ###\n",
    "    # K = np.arange(1, 10, 1)\n",
    "    \n",
    "    print(\"k:     Fractional variance explained:\")\n",
    "    for k in K:\n",
    "        pca = PCA(n_components=k)\n",
    "        pca.fit(training_data)\n",
    "        print(\"%2s %18s\" % (k, np.sum(pca.explained_variance_ratio_)))\n",
    "### STUDENT END ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      " 1      0.76538335544\n",
      " 2     0.948983856061\n",
      " 3     0.983348751618\n",
      " 4     0.998704657852\n",
      " 5     0.999708153699\n",
      " 6     0.999891486841\n",
      " 7     0.999949681623\n",
      " 8     0.999999429716\n",
      " 9                1.0\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(train_numeric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "train_binary_data = binarize(train_numeric_data)\n",
    "dev_binary_data = binarize(dev_numeric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      " 1     0.439886723617\n",
      " 2     0.603598868173\n",
      " 3     0.763970460573\n",
      " 4     0.877066855752\n",
      " 5     0.931344676798\n",
      " 6     0.961546633226\n",
      " 7     0.987182746176\n",
      " 8     0.996811351306\n",
      " 9                1.0\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(train_binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "def perform_GMM(train_data, train_labels, test_data, test_labels, p_components = np.arange(1, 5, 1)):\n",
    "### STUDENT START ###\n",
    "    g_components = np.arange(1, 4, 1)\n",
    "    covariance = [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_roc_auc = 0\n",
    "    best_accurate_gmm = None\n",
    "    best_roc_auc_gmm = None\n",
    "    for pcom in p_components:\n",
    "        pca = PCA(n_components=pcom)\n",
    "        train_transform = pca.fit_transform(train_data)\n",
    "        test_transform = pca.transform(test_data)\n",
    "        for gcom, cov in [(gcom,cov) for gcom in g_components for cov in covariance]:\n",
    "            gmm = GMM(n_components=gcom, covariance_type=cov)\n",
    "            gmm.fit(train_transform)\n",
    "            \n",
    "            test_preds = gmm.predict(test_transform)\n",
    "            \n",
    "            accuracy = metrics.accuracy_score(test_labels, gmm.predict(test_transform))\n",
    "            \n",
    "            roc_auc = metrics.roc_auc_score(test_labels, gmm.predict(test_transform))\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_accurate_gmm = gmm\n",
    "                ac_roc_auc = roc_auc\n",
    "            if roc_auc > best_roc_auc:\n",
    "                best_roc_auc = roc_auc\n",
    "                best_roc_auc_gmm = gmm\n",
    "                ra_acc = accuracy\n",
    "                \n",
    "\n",
    "    print(\"Best Test accuracy: %s\\n\" % (best_accuracy))\n",
    "    print(\"roc_auc of gmm classifier: %s\\n\" % (ac_roc_auc))\n",
    "    print(\"Best accurate gmm classifier: \\n%s\\n\" % (best_accurate_gmm))\n",
    "\n",
    "    \n",
    "    print(\"Best Test roc_auc: %s\\n\" % (best_roc_auc))\n",
    "    print(\"Accuracy of gmm classifier: %s\\n\" % (ra_acc))\n",
    "    print(\"Best accurate gmm classifier: \\n%s\\n\" % (best_roc_auc_gmm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.59190188172\n",
      "\n",
      "Accuracy of gmm classifier: 0.352\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='full', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(train_numeric_data, train_labels, dev_numeric_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.59765625\n",
      "\n",
      "Accuracy of gmm classifier: 0.356\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(train_binary_data, train_labels, dev_binary_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gmm_PosNeg(train_data, train_labels, test_data, test_labels, p_components = np.arange(1, 5, 1)):\n",
    "    train_data, test_data = np.asarray(train_data), np.asarray(test_data)\n",
    "    train_labels, test_labels = np.asarray(train_labels), np.asarray(test_labels) \n",
    "    g_components = np.arange(1, 4, 1)\n",
    "    covariance = [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_gmm = None\n",
    "    best_roc_auc = 0\n",
    "    for pcom in p_components:\n",
    "        pca = PCA(n_components=pcom)\n",
    "        train_transform = pca.fit_transform(train_data)\n",
    "        test_transform = pca.transform(test_data)\n",
    "        \n",
    "        positive_train = train_transform[np.where(train_labels==True)] \n",
    "        negative_train = train_transform[np.where(train_labels==False)]\n",
    "        \n",
    "        for gcom, cov in [(gcom,cov) for gcom in g_components for cov in covariance]:\n",
    "            gmm_positive = GMM(n_components=gcom, covariance_type=cov)\n",
    "            gmm_positive.fit(positive_train)\n",
    "\n",
    "            gmm_negative = GMM(n_components=gcom, covariance_type=cov)\n",
    "            gmm_negative.fit(negative_train)\n",
    "            \n",
    "            pos_scores = gmm_positive.score(test_transform)\n",
    "            neg_scores = gmm_negative.score(test_transform)\n",
    "            \n",
    "            pred_labels = [1 if pos_scores[i] > neg_scores[i] else 0 for (i, d) in enumerate(test_transform)]\n",
    "            pred_labels = np.asarray(pred_labels)\n",
    "            accuracy = np.mean(pred_labels.ravel() == test_labels.ravel()) * 100\n",
    "            \n",
    "            roc_auc = metrics.roc_auc_score(test_labels, pred_labels)\n",
    "            \n",
    "            roc_auc_pos = metrics.roc_auc_score(test_labels, gmm_positive.predict(test_transform))\n",
    "            roc_auc_neg = metrics.roc_auc_score(test_labels, gmm_negative.predict(test_transform))\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                acc_roc = roc_auc\n",
    "                acc_roc_pos = roc_auc_pos\n",
    "                acc_roc_neg = roc_auc_neg\n",
    "                best_gmm = gmm_positive\n",
    "            \n",
    "            if  roc_auc > best_roc_auc:\n",
    "                best_roc_auc = roc_auc\n",
    "                roc_accuracy = accuracy\n",
    "                roc_accuracy_pos = roc_auc_pos\n",
    "                roc_accuracy_neg = roc_auc_neg                \n",
    "                roc_gmm = gmm_positive\n",
    "\n",
    "    print(\"Best Test accuracy: %s\\n\" % (best_accuracy))\n",
    "    print(\"gmm classifier: \\n%s\" % (best_gmm))    \n",
    "    print(\"roc auc: \\n%s\\n\" % (acc_roc))\n",
    "    print(\"pos roc auc: \\n%s\\n\" % (acc_roc_pos))    \n",
    "    print(\"neg roc auc: \\n%s\\n\" % (acc_roc_neg))\n",
    "    \n",
    "    print(\"Best roc auc: \\n%s\" % (best_roc_auc))\n",
    "    print(\"gmm classifier: \\n%s\" % (roc_gmm))      \n",
    "    print(\"Accuracy: %s\\n\" % (roc_accuracy))  \n",
    "    print(\"pos roc auc: %s\\n\" % (roc_accuracy_pos))  \n",
    "    print(\"neg roc auc: %s\\n\" % (roc_accuracy_neg))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 70.6\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=2, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.525705645161\n",
      "\n",
      "pos roc auc: \n",
      "0.49470766129\n",
      "\n",
      "neg roc auc: \n",
      "0.495925739247\n",
      "\n",
      "Best roc auc: \n",
      "0.571824596774\n",
      "gmm classifier: \n",
      "GMM(covariance_type='full', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 58.4\n",
      "\n",
      "pos Accuracy: 0.558635752688\n",
      "\n",
      "neg Accuracy: 0.532888104839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(train_numeric_data, train_labels, dev_numeric_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 70.2\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.57169858871\n",
      "\n",
      "pos roc auc: \n",
      "0.5\n",
      "\n",
      "neg roc auc: \n",
      "0.5\n",
      "\n",
      "Best roc auc: \n",
      "0.686071908602\n",
      "gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 63.2\n",
      "\n",
      "pos Accuracy: 0.5\n",
      "\n",
      "neg Accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(train_binary_data, train_labels, dev_binary_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4671L, 13669L)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "print(tdf.shape)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 58.8\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.597572244624\n",
      "\n",
      "pos roc auc: \n",
      "0.5\n",
      "\n",
      "neg roc auc: \n",
      "0.5\n",
      "\n",
      "Best roc auc: \n",
      "0.605132728495\n",
      "gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 58.4\n",
      "\n",
      "pos Accuracy: 0.5\n",
      "\n",
      "neg Accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(tdf, train_labels, ddf, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.601709509409\n",
      "\n",
      "Accuracy of gmm classifier: 0.462\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(tdf, train_labels, ddf, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf_binary = binarize(tdf)\n",
    "ddf_binary = binarize(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 64.4\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.614709341398\n",
      "\n",
      "pos roc auc: \n",
      "0.5\n",
      "\n",
      "neg roc auc: \n",
      "0.5\n",
      "\n",
      "Best roc auc: \n",
      "0.614709341398\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 64.4\n",
      "\n",
      "pos Accuracy: 0.5\n",
      "\n",
      "neg Accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(tdf_binary, train_labels, ddf_binary, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.632938508065\n",
      "\n",
      "Accuracy of gmm classifier: 0.31\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(tdf_binary, train_labels, ddf_binary, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "100     0.250625555279\n",
      "200     0.366817815376\n",
      "300     0.449003138972\n",
      "400     0.512314000251\n",
      "500     0.563696355339\n",
      "600     0.606643021743\n",
      "700     0.643378817516\n",
      "800     0.675381817972\n",
      "900     0.703665974325\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf, K=np.arange(100, 1000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "1000      0.72894345897\n",
      "1200     0.772477338559\n",
      "1400     0.808713532768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyboardInterrupt in 'zmq.backend.cython.message.Frame.__dealloc__' ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-ab08d940234c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperform_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-136-3a6a0c52ee81>\u001b[0m in \u001b[0;36mperform_PCA\u001b[1;34m(training_data, K)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%2s %18s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m### STUDENT END ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mexplained_variance_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         explained_variance_ratio_ = (explained_variance_ /\n\u001b[0;32m    277\u001b[0m                                      explained_variance_.sum())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf, K=np.arange(1000, 1400, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "200     0.567600356131\n",
      "400     0.722892908662\n",
      "600     0.804466683698\n",
      "800     0.855073456175\n",
      "1000     0.889495778778\n",
      "1200      0.91425701478\n",
      "1400     0.932798529542\n",
      "1600     0.947055924804\n",
      "1800      0.95822866246\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf_binary, K=np.arange(200, 2000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 13669L)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', norm='l1')\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "tdf_binary = binarize(tdf)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()\n",
    "ddf_binary = binarize(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "1400       0.9092050051\n",
      "1600     0.927980371388\n",
      "1800     0.942889553312\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf_binary, K=np.arange(1400, 2000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.60294858871\n",
      "\n",
      "Accuracy of gmm classifier: 0.58\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(tdf_binary, train_labels, ddf_binary, dev_labels, p_components=np.arange(1, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742\n",
      "0.514028897849\n",
      "0.824234639264\n",
      "0.645736307285\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=100)\n",
    "tdf_pca = pca.fit_transform(tdf_binary)\n",
    "ddf_pca = pca.transform(ddf_binary)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(tdf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(ddf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(ddf_pca))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(tdf_pca)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(tdf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744 0.5\n",
      "0.75294369514 0.5\n"
     ]
    }
   ],
   "source": [
    "gmm = GMM()\n",
    "\n",
    "gmm.fit(tdf_binary)\n",
    "\n",
    "dev_accuracy = metrics.accuracy_score(dev_labels, gmm.predict(ddf_binary))\n",
    "dev_roc_auc = metrics.roc_auc_score(dev_labels, gmm.predict(ddf_binary))\n",
    "\n",
    "train_accuracy = metrics.accuracy_score(train_labels, gmm.predict(tdf_binary))\n",
    "train_roc_auc = metrics.roc_auc_score(train_labels, gmm.predict(tdf_binary))\n",
    "\n",
    "print(dev_accuracy, dev_roc_auc)\n",
    "print(train_accuracy, train_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744\n",
      "0.512810819892\n",
      "0.788696210662\n",
      "0.573230345896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(tdf_binary, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(ddf_binary)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(ddf_binary))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(tdf_binary)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(tdf_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "tdf_pca = pca.fit_transform(tdf_binary)\n",
    "ddf_pca = pca.transform(ddf_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "train_binary_data = binarize(train_numeric_data)\n",
    "\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_binary_data = binarize(dev_numeric_data)\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.527091733871\n",
      "0.75594091201\n",
      "0.519456869309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_binary_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(dev_binary_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(dev_binary_data))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(train_binary_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(train_binary_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 100L)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 9L)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm='l2')\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "tdf_binary = binarize(tdf)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()\n",
    "ddf_binary = binarize(ddf)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "tdf_pca = pca.fit_transform(tdf_binary)\n",
    "ddf_pca = pca.transform(ddf_binary)\n",
    "\n",
    "train_combo = np.concatenate((tdf_pca, train_numeric_data), axis=1)\n",
    "dev_combo = np.concatenate((ddf_pca, dev_numeric_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "0.642053091398\n",
      "0.835795332905\n",
      "0.718621708178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_combo, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(dev_combo))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(train_combo)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(train_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804973596647\n",
      "{'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.005, 0.01, 0.02],\n",
    "    'max_depth': [2, 3, 4],\n",
    "}\n",
    "\n",
    "gmm = GradientBoostingClassifier(random_state=0)\n",
    "gsc = GridSearchCV(gmm, param_grid, cv=6, verbose=0, scoring='roc_auc')\n",
    "gsc.fit(train_combo, train_labels)\n",
    "\n",
    "print (gsc.best_score_)\n",
    "print (gsc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n",
      "0.623865927419\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(dev_labels, gsc.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gsc.predict(dev_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n",
      "0.645959341398\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n",
      "0.513902889785\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(tdf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(ddf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(ddf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selected numeric columns from train_data and dev_data\n",
    "# number represents the column index starting at 0\n",
    "# 2 \t number_of_downvotes_of_request_at_retrieval\n",
    "# 6 \t request_number_of_comments_at_retrieval\n",
    "# 10\t requester_account_age_in_days_at_request\n",
    "# 12\t requester_days_since_first_post_on_raop_at_request\n",
    "# 14\t requester_number_of_comments_at_request\n",
    "# 16\t requester_number_of_comments_in_raop_at_request\n",
    "# 18\t requester_number_of_posts_at_request\n",
    "# 20\t requester_number_of_posts_on_raop_at_request\n",
    "# 22\t requester_number_of_subreddits_at_request\n",
    "\n",
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n",
      "0.610803091398\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.521967405914\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_binary_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_binary_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_binary_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758\n",
      "0.573462701613\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((tdf_binary, train_binary_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((ddf_binary, dev_binary_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selected numeric columns from train_data and dev_data\n",
    "# number represents the column index starting at 0\n",
    "# 2 \t number_of_downvotes_of_request_at_retrieval\n",
    "# 6 \t request_number_of_comments_at_retrieval\n",
    "# 10\t requester_account_age_in_days_at_request\n",
    "# 12\t requester_days_since_first_post_on_raop_at_request\n",
    "# 14\t requester_number_of_comments_at_request\n",
    "# 16\t requester_number_of_comments_in_raop_at_request\n",
    "# 18\t requester_number_of_posts_at_request\n",
    "# 20\t requester_number_of_posts_on_raop_at_request\n",
    "# 22\t requester_number_of_subreddits_at_request\n",
    "train_numeric_data = train_data[:, [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n",
      "0.712239583333\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746\n",
      "0.578209005376\n"
     ]
    }
   ],
   "source": [
    "train_binary = binarize(train_numeric_data)\n",
    "dev_binary = binarize(dev_numeric_data)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_binary, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_binary)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      " 1     0.999999994097\n",
      " 2     0.999999999561\n",
      " 3     0.999999999939\n",
      " 4     0.999999999991\n",
      " 5     0.999999999997\n",
      " 6     0.999999999999\n",
      " 7     0.999999999999\n",
      " 8                1.0\n",
      " 9                1.0\n",
      "10                1.0\n",
      "11                1.0\n",
      "12                1.0\n",
      "13                1.0\n",
      "14                1.0\n",
      "15                1.0\n",
      "16                1.0\n",
      "17                1.0\n",
      "18                1.0\n",
      "19                1.0\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(train_numeric_data, K=np.arange(1, 20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714\n",
      "0.510584677419\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "train_num_pca = pca.fit_transform(train_numeric_data)\n",
    "dev_num_pca = pca.transform(dev_numeric_data)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_num_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_num_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_num_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n",
      "0.710769489247\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((tdf_pca, train_numeric_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((ddf_pca, dev_numeric_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.724336357527\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((tdf_binary, train_numeric_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((ddf_binary, dev_numeric_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n",
      "0.519027217742\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_title, dev_title = train_data[:, 9], dev_data[:, 9]\n",
    "train_titf, dev_titf = vectorizer.fit_transform(train_title).toarray(), vectorizer.transform(dev_title).toarray()\n",
    "\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_titf, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_titf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_titf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n",
      "0.99609375\n"
     ]
    }
   ],
   "source": [
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "test_data, test_labels = np.delete(pizza_df[500:1000], 23, axis=1), [x for x in pizza_df[500:1000, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# train_flair, dev_flair = train_data[:, 28], dev_data[:, 28]\n",
    "\n",
    "\n",
    "train_flair = [str(x) for x in train_data[:, 28]]\n",
    "dev_flair = [str(x) for x in dev_data[:, 28]]\n",
    "\n",
    "\n",
    "train_flaf, dev_flaf = vectorizer.fit_transform(train_flair).toarray(), vectorizer.transform(dev_flair).toarray()\n",
    "\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_flaf, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_flaf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_flaf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n",
      "0.511214717742\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# print(' '.join(train_data[1, 23]))\n",
    "\n",
    "# train_subreddits, dev_subreddits = train_data[:, 23], dev_data[:, 23]\n",
    "\n",
    "train_subreddits = [str(x) for x in train_data[:, 23]]\n",
    "\n",
    "dev_subreddits = [str(x) for x in dev_data[:, 23]]\n",
    "\n",
    "\n",
    "train_subf, dev_subf = vectorizer.fit_transform(train_subreddits).toarray(), vectorizer.transform(dev_subreddits).toarray()\n",
    "\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_subf, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_subf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_subf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "100     0.323573550518\n",
      "200     0.455603755206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-f914f206283a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperform_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_titf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-136-3a6a0c52ee81>\u001b[0m in \u001b[0;36mperform_PCA\u001b[1;34m(training_data, K)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%2s %18s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m### STUDENT END ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mexplained_variance_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         explained_variance_ratio_ = (explained_variance_ /\n\u001b[0;32m    277\u001b[0m                                      explained_variance_.sum())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_PCA(train_titf, K=np.arange(100, 1000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728\n",
      "0.509744623656\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=700)\n",
    "train_sub_pca, dev_sub_pca = pca.fit_transform(train_subf), pca.transform(dev_subf)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_sub_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_sub_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_sub_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.502310147849\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=700)\n",
    "train_titf_pca, dev_titf_pca = pca.fit_transform(train_titf), pca.transform(dev_titf)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_titf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_titf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_titf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n",
      "0.506090389785\n"
     ]
    }
   ],
   "source": [
    "train_subtitf_pca = np.concatenate((train_titf_pca, train_sub_pca), axis=1)\n",
    "dev_subtitf_pca = np.concatenate((dev_titf_pca, dev_sub_pca), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_subtitf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_subtitf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_subtitf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.525369623656\n"
     ]
    }
   ],
   "source": [
    "train_all_pca = np.concatenate((train_subtitf_pca, tdf_pca), axis=1)\n",
    "dev_all_pca = np.concatenate((dev_subtitf_pca, ddf_pca), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_all_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_all_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_all_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818\n",
      "0.716271841398\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((train_all_pca, train_numeric_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((dev_all_pca, dev_numeric_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-316-745aa3a2d734>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', norm='l1')\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "tdf_binary = binarize(tdf)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()\n",
    "ddf_binary = binarize(ddf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use Kaggle's Random Accts of Pizza classification task for my project.\n",
    "\n",
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "\n",
    "Let's start by importing some libraries and loading the data into an ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import urllib\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# download the data and extract the tarball \n",
    "tf = urllib.URLopener()\n",
    "tf.retrieve(\"http://cs.stanford.edu/~althoff/raop-dataset/pizza_request_dataset.tar.gz\", \"pizza.tar.gz\")\n",
    "\n",
    "tar = tarfile.open(\"pizza.tar.gz\", \"r:gz\")\n",
    "for name in tar.getnames():\n",
    "    if name == \"pizza_request_dataset/pizza_request_dataset.json\":\n",
    "        member = tar.getmember(name)\n",
    "        f = tar.extractfile(member)\n",
    "        if f is not None:\n",
    "            json_data = f.read()\n",
    "\n",
    "# convert data to a pandas dataframe\n",
    "pizza_df = pd.read_json(json_data)\n",
    "# print(pizza_df[:0])\n",
    "pizza_df = np.asarray(pizza_df)\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(pizza_df.shape[0]))\n",
    "pizza_df = pizza_df[shuffle]\n",
    "\n",
    "# extract test and train data and labels\n",
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "test_data, test_labels = np.delete(pizza_df[500:1000], 23, axis=1), [x for x in pizza_df[500:1000, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a baseline\n",
    "\n",
    "Let's establish a baseline using BernoulliNB, MultinomialNB and LogisticRegression. We will use just the post text (corresponding to column 7 in pizza_df) and the \"requester_received_pizza\" outcome (True, False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define a helper function to perform the analysis \n",
    "def perform_analysis(train_data, train_labels, dev_data, dev_features, \n",
    "                     vectorizer=CountVectorizer(), clf = BernoulliNB(), \n",
    "                     gsc_params = {}):\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(train_data)\n",
    "    dev_data_features = vectorizer.transform(dev_data)\n",
    "    \n",
    "    print(\"RESULTS FOR Default %s : -------------------------------\" % (clf))\n",
    "    clf.fit(train_data_features, train_labels)\n",
    "    print(\"f1_score: %s\\naccuracy_score: %s\\nroc_auc_score: %s\\n\" \n",
    "              % (metrics.f1_score(dev_labels, clf.predict(dev_data_features)), \n",
    "          metrics.accuracy_score(dev_labels, clf.predict(dev_data_features)), \n",
    "                metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))\n",
    "    \n",
    "    print(\"Calculating Cross Vaidation Scores: \")\n",
    "    scores = cross_validation.cross_val_score(clf, train_data_features, train_labels, cv=10, scoring='f1_weighted')\n",
    "    print(\"Scores: %s\\n\" % (scores))\n",
    "    \n",
    "    # Search for the best estimator\n",
    "    print(\"STARTING GRID SEARCH...\")\n",
    "    gsc = GridSearchCV(clf, gsc_params, n_jobs=-1)\n",
    "    gsc.fit(train_data_features, train_labels)\n",
    "    print(\"Best estimator: %s\\nBest alpha: %s\\nBest score: %s\\nScorer function: %s\\n\" \n",
    "          % (gsc.best_estimator_, gsc.best_params_, gsc.best_score_, gsc.scorer_))\n",
    "    # return gsc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB using CountVectorizer\n",
      "RESULTS FOR Default BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.305555555556\n",
      "accuracy_score: 0.7\n",
      "roc_auc_score: 0.554981518817\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.68923924  0.69364719  0.68049658  0.67924009  0.69507729  0.69611298\n",
      "  0.67960775  0.66109026  0.68012268  0.67949566]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: BernoulliNB(alpha=0.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.0}\n",
      "Best score: 0.737315350032\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n",
      "BernoulliNB using TfidfVectorizer\n",
      "RESULTS FOR Default BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.212290502793\n",
      "accuracy_score: 0.718\n",
      "roc_auc_score: 0.531207997312\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.6948353   0.69313005  0.70509014  0.69313005  0.70155229  0.67149621\n",
      "  0.67980691  0.65757883  0.68676729  0.67902348]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: BernoulliNB(alpha=0.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.0}\n",
      "Best score: 0.736887176194\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB using CountVectorizer\n",
    "print(\"BernoulliNB using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = BernoulliNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})\n",
    "\n",
    "# BernoulliNB using TfidfVectorizer\n",
    "print(\"BernoulliNB using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'), clf = BernoulliNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB using CountVectorizer\n",
      "RESULTS FOR Default MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.0915032679739\n",
      "accuracy_score: 0.722\n",
      "roc_auc_score: 0.503150201613\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.69658503  0.67541664  0.67977306  0.67645383  0.69491832  0.6602577\n",
      "  0.6839763   0.63693178  0.64502397  0.6718563 ]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: MultinomialNB(alpha=0.98999999999999999, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.98999999999999999}\n",
      "Best score: 0.731749090131\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n",
      "MultinomialNB using TfidfVectorizer\n",
      "RESULTS FOR Default MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.0\n",
      "accuracy_score: 0.744\n",
      "roc_auc_score: 0.5\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.64573692  0.64468864  0.64573692  0.64573692  0.64790979  0.64790979\n",
      "  0.64790979  0.64719664  0.64719664  0.64719664]\n",
      "\n",
      "STARTING GRID SEARCH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: MultinomialNB(alpha=0.91000000000000003, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.91000000000000003}\n",
      "Best score: 0.75294369514\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB using CountVectorizer\n",
    "print(\"MultinomialNB using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = MultinomialNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})\n",
    "\n",
    "# MultinomialNB using TfidfVectorizer\n",
    "print(\"MultinomialNB using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'), clf = MultinomialNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression using CountVectorizer\n",
      "RESULTS FOR Default LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) : -------------------------------\n",
      "f1_score: 0.209523809524\n",
      "accuracy_score: 0.668\n",
      "roc_auc_score: 0.50529233871\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.71056113  0.70374342  0.68475426  0.70022931  0.66436359  0.68405106\n",
      "  0.69450078  0.657613    0.68159917  0.70261076]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: LogisticRegression(C=0.02, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best alpha: {'C': 0.02}\n",
      "Best score: 0.754656390495\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n",
      "LogisticRegression using TfidfVectorizer\n",
      "RESULTS FOR Default LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) : -------------------------------\n",
      "f1_score: 0.0296296296296\n",
      "accuracy_score: 0.738\n",
      "roc_auc_score: 0.501092069892\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.66988649  0.67487664  0.65482947  0.67362827  0.66645446  0.66645446\n",
      "  0.66874512  0.65741875  0.67469176  0.66339267]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: LogisticRegression(C=0.76000000000000001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best alpha: {'C': 0.76000000000000001}\n",
      "Best score: 0.75594091201\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression using CountVectorizer\n",
    "print(\"LogisticRegression using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = LogisticRegression(), \n",
    "                 gsc_params = {'C': np.arange(0.01, 1, 0.01)})\n",
    "\n",
    "# LogisticRegression using TfidfVectorizer\n",
    "print(\"LogisticRegression using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(), clf = LogisticRegression(), \n",
    "                 gsc_params = {'C': np.arange(0.01, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier using CountVectorizer\n",
      "RESULTS FOR Default KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') : -------------------------------\n",
      "f1_score: 0.103896103896\n",
      "accuracy_score: 0.724\n",
      "roc_auc_score: 0.507056451613\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.66880342  0.65150464  0.67415834  0.66070829  0.65334379  0.66829823\n",
      "  0.64546451  0.65698709  0.66896721  0.64462871]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=52, p=2,\n",
      "           weights='uniform')\n",
      "Best alpha: {'n_neighbors': 52}\n",
      "Best score: 0.75315778206\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n",
      "KNeighborsClassifier using TfidfVectorizer\n",
      "RESULTS FOR Default KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') : -------------------------------\n",
      "f1_score: 0.0285714285714\n",
      "accuracy_score: 0.728\n",
      "roc_auc_score: 0.494371639785\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.65019014  0.66067982  0.65678407  0.64669785  0.6409107   0.64967935\n",
      "  0.63943053  0.64476271  0.64362025  0.64296913]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "           weights='uniform')\n",
      "Best alpha: {'n_neighbors': 12}\n",
      "Best score: 0.75294369514\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier using CountVectorizer\n",
    "print(\"KNeighborsClassifier using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = KNeighborsClassifier(), \n",
    "                 gsc_params = {'n_neighbors': np.arange(1, 100, 1)})\n",
    "\n",
    "# KNeighborsClassifier using TfidfVectorizer\n",
    "print(\"KNeighborsClassifier using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(), clf = KNeighborsClassifier(), \n",
    "                 gsc_params = {'n_neighbors': np.arange(1, 100, 1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, using default options with KNearestNeighbors, BernoulliNB, MultinomialNB and LogisticRegression yielded accuracy better that random guessing. All three classifiers scored between approximately 70% to 75%. We can use this as a baseline and look to improve the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better baseline?\n",
    "\n",
    "I suppose 75% accuracy is a good baseline to start with. Let's look to improve this in the next steps. I will try a pipeline as the first attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline (chaining classifiers)\n",
    "\n",
    "Let's see if chained transformation via CountVectorizer and TfiddfTransformer, followed by LogisticRegression, improves the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   26.8s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   32.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best alpha: {'vect__ngram_range': (1, 1), 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "Best score: 0.756797259687\n",
      "Scorer function: <function _passthrough_scorer at 0x000000000835A898>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# let's learn about pipelines; they will come in handy for ensembles\n",
    "# ****code below is taken from scikit-learn's documentation on pipelines****\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "# **** end of code taken from scikit-learn's documentation ****\n",
    "\n",
    "gsc = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "gsc.fit(train_data[:, 7], train_labels)\n",
    "print(\"Best estimator: %s\\nBest alpha: %s\\nBest score: %s\\nScorer function: %s\\n\" \n",
    "      % (gsc.best_estimator_, gsc.best_params_, gsc.best_score_, gsc.scorer_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Randomized trees and Ensembles\n",
    "Let's first look at the results of randomized trees and then use an ensemble (the VotingClassifier). Let's also convert user's subreddits to a \"bag of words\" and run that through randomized trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier score: 0.744\n",
      "ExtraTreesClassifier roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000, analyzer='word', norm='l2', stop_words='english')\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf.fit(train_data_features, train_labels)\n",
    "\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(dev_data_features, dev_labels)))\n",
    "\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier score: 0.722\n",
      "ExtraTreesClassifier roc_auc_score: 0.485215053763\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees\n",
    "# With subreddit membership as a bag of words\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000, analyzer='word', norm='l2', stop_words='english')\n",
    "\n",
    "td = [' '.join(x) for x in train_data[:, 23]]\n",
    "dd = [' '.join(x) for x in dev_data[:, 23]]\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(td)\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "dev_data_features = vectorizer.transform(dd)\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf.fit(train_data_features, train_labels)\n",
    "\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(dev_data_features, dev_labels)))\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.748\n",
      "VotingClassifier roc_auc_score: 0.512936827957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = BernoulliNB()\n",
    "clf3 = LogisticRegression()\n",
    "clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "vclf1 = VotingClassifier(estimators=[('knn', clf1), ('bnb', clf2), ('lr', clf3), ('etc', clf4)], voting='hard')\n",
    "\n",
    "vclf1.fit(train_data_features, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf1.predict(dev_data_features))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf1.predict(dev_data_features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm surprised to see that the ExtraTreesClassifier returns 72.2% accuracy for the subreddit membership which is marginally lower than the accuracy from the post itself (74%).\n",
    "However, the overall accuracy is similar to the results of individual classifiers used earlier. It appears that a bag of words approve alone is not going to cut it. \n",
    "\n",
    "Let's look to improve upon this by first using a FeatureUnion on post words and subreddits (bag of words, yet again). After that I will look to expand the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Unions to improve the baseline\n",
    "\n",
    "\n",
    "... to be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

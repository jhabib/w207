{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use Kaggle's Random Accts of Pizza classification task for my project.\n",
    "\n",
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "\n",
    "Let's start by importing some libraries and loading the data into an ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [giver_username_if_known, in_test_set, number_of_downvotes_of_request_at_retrieval, number_of_upvotes_of_request_at_retrieval, post_was_edited, request_id, request_number_of_comments_at_retrieval, request_text, request_text_edit_aware, request_title, requester_account_age_in_days_at_request, requester_account_age_in_days_at_retrieval, requester_days_since_first_post_on_raop_at_request, requester_days_since_first_post_on_raop_at_retrieval, requester_number_of_comments_at_request, requester_number_of_comments_at_retrieval, requester_number_of_comments_in_raop_at_request, requester_number_of_comments_in_raop_at_retrieval, requester_number_of_posts_at_request, requester_number_of_posts_at_retrieval, requester_number_of_posts_on_raop_at_request, requester_number_of_posts_on_raop_at_retrieval, requester_number_of_subreddits_at_request, requester_received_pizza, requester_subreddits_at_request, requester_upvotes_minus_downvotes_at_request, requester_upvotes_minus_downvotes_at_retrieval, requester_upvotes_plus_downvotes_at_request, requester_upvotes_plus_downvotes_at_retrieval, requester_user_flair, requester_username, unix_timestamp_of_request, unix_timestamp_of_request_utc]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import urllib\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# download the data and extract the tarball \n",
    "tf = urllib.URLopener()\n",
    "\n",
    "# change the url to http from https if you get a urllib error\n",
    "tf.retrieve(\"https://cs.stanford.edu/~althoff/raop-dataset/pizza_request_dataset.tar.gz\", \"pizza.tar.gz\")\n",
    "\n",
    "tar = tarfile.open(\"pizza.tar.gz\", \"r:gz\")\n",
    "for name in tar.getnames():\n",
    "    if name == \"pizza_request_dataset/pizza_request_dataset.json\":\n",
    "        member = tar.getmember(name)\n",
    "        f = tar.extractfile(member)\n",
    "        if f is not None:\n",
    "            json_data = f.read()\n",
    "\n",
    "# convert data to a pandas dataframe\n",
    "pizza_df = pd.read_json(json_data)\n",
    "print(pizza_df[:0])\n",
    "pizza_df = np.asarray(pizza_df)\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(pizza_df.shape[0]))\n",
    "pizza_df = pizza_df[shuffle]\n",
    "\n",
    "# extract test and train data and labels\n",
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "test_data, test_labels = np.delete(pizza_df[500:1000], 23, axis=1), [x for x in pizza_df[500:1000, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a baseline\n",
    "\n",
    "Let's establish a baseline using BernoulliNB, MultinomialNB and LogisticRegression. We will use just the post text (corresponding to column 7 in pizza_df) and the \"requester_received_pizza\" outcome (True, False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# define a helper function to perform the analysis \n",
    "def perform_analysis(train_data, train_labels, dev_data, dev_features, \n",
    "                     vectorizer=CountVectorizer(), clf = BernoulliNB(), \n",
    "                     gsc_params = {}):\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(train_data)\n",
    "    dev_data_features = vectorizer.transform(dev_data)\n",
    "    \n",
    "    print(\"RESULTS FOR Default %s : -------------------------------\" % (clf))\n",
    "    clf.fit(train_data_features, train_labels)\n",
    "    print(\"f1_score: %s\\naccuracy_score: %s\\nroc_auc_score: %s\\n\" \n",
    "              % (metrics.f1_score(dev_labels, clf.predict(dev_data_features)), \n",
    "          metrics.accuracy_score(dev_labels, clf.predict(dev_data_features)), \n",
    "                metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))\n",
    "    \n",
    "    print(\"Calculating Cross Vaidation Scores: \")\n",
    "    scores = cross_validation.cross_val_score(clf, train_data_features, train_labels, cv=10, scoring='f1_weighted')\n",
    "    print(\"Scores: %s\\n\" % (scores))\n",
    "    \n",
    "    # Search for the best estimator\n",
    "    print(\"STARTING GRID SEARCH...\")\n",
    "    gsc = GridSearchCV(clf, gsc_params, n_jobs=-1)\n",
    "    gsc.fit(train_data_features, train_labels)\n",
    "    print(\"Best estimator: %s\\nBest alpha: %s\\nBest score: %s\\nScorer function: %s\\n\" \n",
    "          % (gsc.best_estimator_, gsc.best_params_, gsc.best_score_, gsc.scorer_))\n",
    "    # return gsc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB using CountVectorizer\n",
      "RESULTS FOR Default BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.305555555556\n",
      "accuracy_score: 0.7\n",
      "roc_auc_score: 0.554981518817\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.68923924  0.69364719  0.68049658  0.67924009  0.69507729  0.69611298\n",
      "  0.67960775  0.66109026  0.68012268  0.67949566]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: BernoulliNB(alpha=0.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.0}\n",
      "Best score: 0.737315350032\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "BernoulliNB using TfidfVectorizer\n",
      "RESULTS FOR Default BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.212290502793\n",
      "accuracy_score: 0.718\n",
      "roc_auc_score: 0.531207997312\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.6948353   0.69313005  0.70509014  0.69313005  0.70155229  0.67149621\n",
      "  0.67980691  0.65757883  0.68676729  0.67902348]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: BernoulliNB(alpha=0.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.0}\n",
      "Best score: 0.736887176194\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB using CountVectorizer\n",
    "print(\"BernoulliNB using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = BernoulliNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})\n",
    "\n",
    "# BernoulliNB using TfidfVectorizer\n",
    "print(\"BernoulliNB using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'), clf = BernoulliNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB using CountVectorizer\n",
      "RESULTS FOR Default MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.0915032679739\n",
      "accuracy_score: 0.722\n",
      "roc_auc_score: 0.503150201613\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.69658503  0.67541664  0.67977306  0.67645383  0.69491832  0.6602577\n",
      "  0.6839763   0.63693178  0.64502397  0.6718563 ]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: MultinomialNB(alpha=0.98999999999999999, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.98999999999999999}\n",
      "Best score: 0.731749090131\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "MultinomialNB using TfidfVectorizer\n",
      "RESULTS FOR Default MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) : -------------------------------\n",
      "f1_score: 0.0\n",
      "accuracy_score: 0.744\n",
      "roc_auc_score: 0.5\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.64573692  0.64468864  0.64573692  0.64573692  0.64790979  0.64790979\n",
      "  0.64790979  0.64719664  0.64719664  0.64719664]\n",
      "\n",
      "STARTING GRID SEARCH...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: MultinomialNB(alpha=0.91000000000000003, class_prior=None, fit_prior=True)\n",
      "Best alpha: {'alpha': 0.91000000000000003}\n",
      "Best score: 0.75294369514\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB using CountVectorizer\n",
    "print(\"MultinomialNB using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = MultinomialNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})\n",
    "\n",
    "# MultinomialNB using TfidfVectorizer\n",
    "print(\"MultinomialNB using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'), clf = MultinomialNB(), \n",
    "                 gsc_params = {'alpha': np.arange(0, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression using CountVectorizer\n",
      "RESULTS FOR Default LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) : -------------------------------\n",
      "f1_score: 0.209523809524\n",
      "accuracy_score: 0.668\n",
      "roc_auc_score: 0.50529233871\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.71056113  0.70374342  0.68475426  0.70022931  0.66436359  0.68405106\n",
      "  0.69450078  0.657613    0.68159917  0.70261076]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: LogisticRegression(C=0.02, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best alpha: {'C': 0.02}\n",
      "Best score: 0.754656390495\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "LogisticRegression using TfidfVectorizer\n",
      "RESULTS FOR Default LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False) : -------------------------------\n",
      "f1_score: 0.0296296296296\n",
      "accuracy_score: 0.738\n",
      "roc_auc_score: 0.501092069892\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.66988649  0.67487664  0.65482947  0.67362827  0.66645446  0.66645446\n",
      "  0.66874512  0.65741875  0.67469176  0.66339267]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: LogisticRegression(C=0.76000000000000001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best alpha: {'C': 0.76000000000000001}\n",
      "Best score: 0.75594091201\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression using CountVectorizer\n",
    "print(\"LogisticRegression using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = LogisticRegression(), \n",
    "                 gsc_params = {'C': np.arange(0.01, 1, 0.01)})\n",
    "\n",
    "# LogisticRegression using TfidfVectorizer\n",
    "print(\"LogisticRegression using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(), clf = LogisticRegression(), \n",
    "                 gsc_params = {'C': np.arange(0.01, 1, 0.01)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier using CountVectorizer\n",
      "RESULTS FOR Default KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') : -------------------------------\n",
      "f1_score: 0.103896103896\n",
      "accuracy_score: 0.724\n",
      "roc_auc_score: 0.507056451613\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.66880342  0.65150464  0.67415834  0.66070829  0.65334379  0.66829823\n",
      "  0.64546451  0.65698709  0.66896721  0.64462871]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=52, p=2,\n",
      "           weights='uniform')\n",
      "Best alpha: {'n_neighbors': 52}\n",
      "Best score: 0.75315778206\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n",
      "KNeighborsClassifier using TfidfVectorizer\n",
      "RESULTS FOR Default KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform') : -------------------------------\n",
      "f1_score: 0.0285714285714\n",
      "accuracy_score: 0.728\n",
      "roc_auc_score: 0.494371639785\n",
      "\n",
      "Calculating Cross Vaidation Scores: \n",
      "Scores: [ 0.65019014  0.66067982  0.65678407  0.64669785  0.6409107   0.64967935\n",
      "  0.63943053  0.64476271  0.64362025  0.64296913]\n",
      "\n",
      "STARTING GRID SEARCH...\n",
      "Best estimator: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=12, p=2,\n",
      "           weights='uniform')\n",
      "Best alpha: {'n_neighbors': 12}\n",
      "Best score: 0.75294369514\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier using CountVectorizer\n",
    "print(\"KNeighborsClassifier using CountVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=CountVectorizer(), clf = KNeighborsClassifier(), \n",
    "                 gsc_params = {'n_neighbors': np.arange(1, 100, 1)})\n",
    "\n",
    "# KNeighborsClassifier using TfidfVectorizer\n",
    "print(\"KNeighborsClassifier using TfidfVectorizer\")\n",
    "perform_analysis(train_data[:, 7], train_labels, dev_data[:, 7], dev_labels, \n",
    "                 vectorizer=TfidfVectorizer(), clf = KNeighborsClassifier(), \n",
    "                 gsc_params = {'n_neighbors': np.arange(1, 100, 1)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, using default options with KNearestNeighbors, BernoulliNB, MultinomialNB and LogisticRegression yielded accuracy better that random guessing. All three classifiers scored between approximately 70% to 75%. We can use this as a baseline and look to improve the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A better baseline?\n",
    "\n",
    "I suppose 75% accuracy is a good baseline to start with. Let's look to improve this in the next steps. I will try a pipeline as the first attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Pipeline (chaining classifiers)\n",
    "\n",
    "Let's see if chained transformation via CountVectorizer and TfiddfTransformer, followed by LogisticRegression, improves the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        st...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Best alpha: {'vect__ngram_range': (1, 1), 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n",
      "Best score: 0.756797259687\n",
      "Scorer function: <function _passthrough_scorer at 0x0000000007E8E0B8>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# let's learn about pipelines; they will come in handy for ensembles\n",
    "# ****code below is taken from scikit-learn's documentation on pipelines****\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (2, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "# **** end of code taken from scikit-learn's documentation ****\n",
    "\n",
    "gsc = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "gsc.fit(train_data[:, 7], train_labels)\n",
    "print(\"Best estimator: %s\\nBest alpha: %s\\nBest score: %s\\nScorer function: %s\\n\" \n",
    "      % (gsc.best_estimator_, gsc.best_params_, gsc.best_score_, gsc.scorer_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Randomized trees and Ensembles\n",
    "Let's first look at the results of randomized trees and then use an ensemble (the VotingClassifier). Let's also convert user's subreddits to a \"bag of words\" and run that through randomized trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier score: 0.744\n",
      "ExtraTreesClassifier roc_auc_score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000, analyzer='word', norm='l2', stop_words='english')\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf.fit(train_data_features, train_labels)\n",
    "\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(dev_data_features, dev_labels)))\n",
    "\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier score: 0.722\n",
      "ExtraTreesClassifier roc_auc_score: 0.485215053763\n",
      "ExtraTreesClassifier score: 0.955469920788\n",
      "ExtraTreesClassifier roc_auc_score: 0.910169791786\n"
     ]
    }
   ],
   "source": [
    "# Extremely Randomized Trees\n",
    "# With subreddit membership as a bag of words\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=1000, analyzer='word', norm='l2', stop_words='english')\n",
    "\n",
    "td = [' '.join(x) for x in train_data[:, 23]]\n",
    "dd = [' '.join(x) for x in dev_data[:, 23]]\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(td)\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "dev_data_features = vectorizer.transform(dd)\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf.fit(train_data_features, train_labels)\n",
    "\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(dev_data_features, dev_labels)))\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, clf.predict(dev_data_features))))\n",
    "print(\"ExtraTreesClassifier score: %s\" % (clf.score(train_data_features, train_labels)))\n",
    "print(\"ExtraTreesClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, clf.predict(train_data_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.748\n",
      "VotingClassifier roc_auc_score: 0.512936827957\n",
      "VotingClassifier accuracy score: 0.868764718476\n",
      "VotingClassifier roc_auc_score: 0.734402079723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "\n",
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = BernoulliNB()\n",
    "clf3 = LogisticRegression()\n",
    "clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "vclf1 = VotingClassifier(estimators=[('knn', clf1), ('bnb', clf2), ('lr', clf3), ('etc', clf4)], voting='hard')\n",
    "\n",
    "vclf1.fit(train_data_features, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf1.predict(dev_data_features))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf1.predict(dev_data_features))))\n",
    "\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(train_labels, vclf1.predict(train_data_features))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, vclf1.predict(train_data_features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm surprised to see that the ExtraTreesClassifier returns 72.2% accuracy for the subreddit membership which is marginally lower than the accuracy from the post itself (74%).\n",
    "However, the overall accuracy is similar to the results of individual classifiers used earlier. It appears that a bag of words approach alone is not going to cut it. \n",
    "\n",
    "Let's look to improve upon this by first using a FeatureUnion on post words and subreddits (bag of words, yet again). After that I will look to expand the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Unions to improve the baseline\n",
    "\n",
    "\n",
    "... to be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline accuracy score: 0.746\n",
      "Pipeline roc_auc_score: 0.50390625\n",
      "Pipeline accuracy score: 1.0\n",
      "Pipeline roc_auc_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ItemSelector class COPIED VERBATIM from:\n",
    "# http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "post_train_data = train_data[:, 7]\n",
    "post_dev_data = dev_data[:, 7] \n",
    "\n",
    "title_train_data = train_data[:, 9]\n",
    "title_dev_data = dev_data[:, 9] \n",
    "\n",
    "\n",
    "subreddits_train_data = [' '.join(x) for x in train_data[:, 23]] \n",
    "subreddits_dev_data = [' '.join(x) for x in dev_data[:, 23]]\n",
    "\n",
    "pipeline_train_data = {'title': title_train_data, \n",
    "                       'posts': post_train_data, \n",
    "                       'subreddits': subreddits_train_data}\n",
    "\n",
    "pipeline_dev_data = {'title': title_dev_data, \n",
    "                     'posts': post_dev_data, \n",
    "                     'subreddits': subreddits_dev_data}\n",
    "pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                    ('title', Pipeline([\n",
    "                                ('selector', ItemSelector(key='title')), \n",
    "                                ('cv', CountVectorizer()), \n",
    "                            ])),\n",
    "                    \n",
    "                    ('subreddits', Pipeline([\n",
    "                                ('selector', ItemSelector(key='subreddits')), \n",
    "                                ('cv', CountVectorizer()), \n",
    "                            ])),\n",
    "                    ('post', Pipeline([\n",
    "                                ('selector', ItemSelector(key='posts')), \n",
    "                                ('vect', TfidfVectorizer(min_df=0.01, stop_words='english')), \n",
    "                                ('best', TruncatedSVD(n_components=50)), \n",
    "                            ])), \n",
    "                    \n",
    "                ], \n",
    "            transformer_weights={\n",
    "                    'title': 0.5,\n",
    "                    'subreddits': 0.4, \n",
    "                    'post': 0.7, \n",
    "                }, \n",
    "            )), \n",
    "        ('clf', ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy'))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(pipeline_train_data, train_labels)\n",
    "\n",
    "print(\"Pipeline accuracy score: %s\" % (metrics.accuracy_score(dev_labels, pipeline.predict(pipeline_dev_data))))\n",
    "print(\"Pipeline roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, pipeline.predict(pipeline_dev_data))))\n",
    "\n",
    "print(\"Pipeline accuracy score: %s\" % (metrics.accuracy_score(train_labels, pipeline.predict(pipeline_train_data))))\n",
    "print(\"Pipeline roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, pipeline.predict(pipeline_train_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now ain't that a b\\*\\*\\*\\*\\!\n",
    "\n",
    "It looks like a bag-of-words approach is not going to cut it. But all is not lost yet. We can start adding some other features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paying it forward, reciprocity, give & take\n",
    "\n",
    "Let's evaluate notions of give and take and reciprocity in the posts. May be the potential for getting something in return affects people's decision to buy some random person a pizza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob. of NOT getting pizza: 0.75\n",
      "Prob. of NOT getting pizza given reciprocity: 0.20 \n",
      "\n",
      "Prob. of getting pizza: 0.25\n",
      "Prob. of getting pizza given reciprocity: 0.26 \n",
      "\n",
      "Prob. of reciprocity in all posts: 0.22 \n",
      "\n",
      "Posterior prob. of getting pizza given reciprocity: 0.29\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "reciprocity_terms = ['pay it forward', 'paying it forward', 'forward',  \n",
    "                     'reciprocate', 'give back', 'trade', 'pay it back'\n",
    "                    'pic', 'pics', 'story', 'stories']\n",
    "\n",
    "def is_phrase_found(phrase_list, target_text):\n",
    "    found = False\n",
    "    for t in reciprocity_terms:\n",
    "        if t in target_text:\n",
    "            found = True\n",
    "            break\n",
    "    return found\n",
    "\n",
    "def get_phrase_freq(phrase_list, target_text):\n",
    "    found = 0\n",
    "    for t in reciprocity_terms:\n",
    "        for d in target_text:\n",
    "            if t in d:\n",
    "                found += 1\n",
    "    return found\n",
    "\n",
    "def calc_odds(data, query_terms):\n",
    "    f = 0.\n",
    "    for d in data:\n",
    "        if(is_phrase_found(query_terms, d)):\n",
    "            f += 1\n",
    "    return f / len(data)\n",
    "\n",
    "train_gotpizza = pizza_df[1000:][pizza_df[1000:][:, 23] == True][:, 7]\n",
    "train_nopizza = pizza_df[1000:][pizza_df[1000:][:, 23] == False][:, 7]\n",
    "dev_gotpizza = pizza_df[:500][pizza_df[:500][:, 23] == True][:, 7]\n",
    "dev_nopizza = pizza_df[:500][pizza_df[:500][:, 23] == False][:, 7]\n",
    "\n",
    "print(\"Prob. of NOT getting pizza: %.2f\" % (len(train_nopizza) / float(len(train_data))))\n",
    "p1 = calc_odds(train_nopizza, reciprocity_terms)\n",
    "print(\"Prob. of NOT getting pizza given reciprocity: %.2f \\n\" % p1)\n",
    "\n",
    "p2 = len(train_gotpizza) / float(len(train_data))\n",
    "print(\"Prob. of getting pizza: %.2f\" % p2)\n",
    "p3 = calc_odds(train_gotpizza, reciprocity_terms)\n",
    "print(\"Prob. of getting pizza given reciprocity: %.2f \\n\" % p3)\n",
    "\n",
    "p4 = calc_odds(train_data[:, 7], reciprocity_terms)\n",
    "print(\"Prob. of reciprocity in all posts: %.2f \\n\" % p4)\n",
    "\n",
    "# Calculate the posterior odds of the desired Event (E), that is getting a pizza\n",
    "# P(E | x) = p(x | E) * p(E) / p(x) \n",
    "print(\"Posterior prob. of getting pizza given reciprocity: %.2f\" % ( p3 * p2 / p4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that reciprocity has a bit of an effect on the likelihood of a post getting a pizza, let's add that as a feature to our pipeline. So may be we can hand adjust the probability of getting a pizza for posts that contain a notion of reciprocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior accuracy score: 0.746\n",
      "Posterior accuracy score: 0.746\n",
      "Posterior roc_auc_score score: 0.506468413978\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "\n",
    "et.fit(train_data_features, train_labels)\n",
    "print(\"Prior accuracy score: %s\" % metrics.accuracy_score(dev_labels, et.predict(dev_data_features)))\n",
    "\n",
    "dev_probs = et.predict_proba(dev_data_features)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    if(is_phrase_found(reciprocity_terms, t)):\n",
    "        dev_probs[i][1] = dev_probs[i][1] * 1.16\n",
    "# write our own prediction\n",
    "dev_predicted_labels = []\n",
    "for i, (f, t) in enumerate(dev_probs):\n",
    "    if(t > f): \n",
    "        dev_predicted_labels.append(True)\n",
    "    else:\n",
    "        dev_predicted_labels.append(False)\n",
    "\n",
    "print(\"Posterior accuracy score: %s\" % metrics.accuracy_score(dev_labels, dev_predicted_labels))\n",
    "print(\"Posterior roc_auc_score score: %s\" % metrics.roc_auc_score(dev_labels, dev_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the simplistic approach of adjusting the probabilities did not work. We can still use this result though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Numeric Features\n",
    "\n",
    "Since the bag of words approach alone did not fare much better than random guessing, at least in the roc_auc_score, let's look at some numeric features. As well as looking at features available directly in the data, we will also calcualte some numeric features: binary code reciprocity and calculate post length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binary code reciprocity\n",
    "reciproc_train_data = np.zeros(len(train_data[:, 7]))\n",
    "reciproc_train_data.reshape(len(train_data[:, 7]), 1)\n",
    "for i, t in enumerate(train_data[:, 7]):\n",
    "    reciproc_train_data[i] = int(is_phrase_found(reciprocity_terms, t))\n",
    "\n",
    "reciproc_dev_data = np.zeros(len(dev_data[:, 7]))\n",
    "reciproc_dev_data.reshape(len(dev_data[:, 7]), 1)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    reciproc_dev_data[i] = int(is_phrase_found(reciprocity_terms, t)) \n",
    "\n",
    "# Calculate Post Length\n",
    "length_train_post = np.zeros(len(train_data[:, 7]))\n",
    "length_train_post.reshape(len(train_data[:, 7]), 1)\n",
    "for i, t in enumerate(train_data[:, 7]):\n",
    "    length_train_post[i] = len(t.split())\n",
    "\n",
    "length_dev_post = np.zeros(len(dev_data[:, 7]))\n",
    "length_dev_post.reshape(len(dev_data[:, 7]), 1)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    length_dev_post[i] = len(t.split())\n",
    "\n",
    "# selected numeric columns from train_data and dev_data\n",
    "# number represents the column index starting at 0\n",
    "# 2 \t number_of_downvotes_of_request_at_retrieval\n",
    "# 6 \t request_number_of_comments_at_retrieval\n",
    "# 10\t requester_account_age_in_days_at_request\n",
    "# 12\t requester_days_since_first_post_on_raop_at_request\n",
    "# 14\t requester_number_of_comments_at_request\n",
    "# 16\t requester_number_of_comments_in_raop_at_request\n",
    "# 18\t requester_number_of_posts_at_request\n",
    "# 20\t requester_number_of_posts_on_raop_at_request\n",
    "# 22\t requester_number_of_subreddits_at_request\n",
    "\n",
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine the numeric data \n",
    "train_temp = []\n",
    "for i in range(len(train_data[:, 7])):\n",
    "    train_temp.append(np.hstack((train_numeric_data[0], reciproc_train_data[0])))\n",
    "train_combined = []\n",
    "for i in range(len(train_data[:, 7])):\n",
    "    train_combined.append(np.hstack((train_temp[0], length_train_post[0])))\n",
    "\n",
    "dev_temp = []\n",
    "for i in range(len(dev_data[:, 7])):\n",
    "    dev_temp.append(np.hstack((dev_numeric_data[0], reciproc_dev_data[0])))\n",
    "dev_combined = []\n",
    "for i in range(len(dev_data[:, 7])):\n",
    "    dev_combined.append(np.hstack((dev_temp[0], length_dev_post[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n",
      "0.58639952957\n",
      "0.769428387925\n",
      "0.575281906304\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, lr.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, lr.predict(dev_numeric_data)))\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, lr.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, lr.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.579175067204\n",
      "0.97580817812\n",
      "0.95453316868\n"
     ]
    }
   ],
   "source": [
    "et = ExtraTreesClassifier()\n",
    "et.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, et.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, et.predict(dev_numeric_data)))\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, et.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, et.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n",
      "0.622521841398\n",
      "0.819738813958\n",
      "0.693112655589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(dev_numeric_data))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_text_features = vectorizer.fit_transform(train_data[:, 7]).todense()\n",
    "dev_text_features = vectorizer.transform(dev_data[:, 7]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_features = np.concatenate((train_text_features, train_numeric_data), axis=1)\n",
    "all_dev_features = np.concatenate((dev_text_features, dev_numeric_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754\n",
      "0.598958333333\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(all_train_features, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(all_dev_features)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(all_dev_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype object was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "atf_normalized = preprocessing.normalize(all_train_features, norm='l2')\n",
    "adf_normalized = preprocessing.normalize(all_dev_features, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724\n",
      "0.496807795699\n"
     ]
    }
   ],
   "source": [
    "gb = ExtraTreesClassifier()\n",
    "gb.fit(atf_normalized, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(adf_normalized)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(adf_normalized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.744\n",
      "VotingClassifier roc_auc_score: 0.505124327957\n"
     ]
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = BernoulliNB()\n",
    "clf3 = LogisticRegression()\n",
    "clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "vclf2 = VotingClassifier(estimators=[('knn', clf1), ('bnb', clf2), ('lr', clf3), ('etc', clf4)], voting='hard')\n",
    "\n",
    "\n",
    "vclf2.fit(atf_normalized, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf2.predict(adf_normalized))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf2.predict(adf_normalized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.746\n",
      "VotingClassifier roc_auc_score: 0.509030577957\n"
     ]
    }
   ],
   "source": [
    "clf5 = GradientBoostingClassifier()\n",
    "vclf3 = VotingClassifier(estimators=[('lr', clf3), ('etc', clf4), ('gb', clf5)], voting='hard')\n",
    "\n",
    "\n",
    "vclf3.fit(atf_normalized, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf3.predict(adf_normalized))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf3.predict(adf_normalized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_PCA(training_data, K=np.arange(1, 10, 1)):\n",
    "### STUDENT START ###\n",
    "    # K = np.arange(1, 10, 1)\n",
    "    \n",
    "    print(\"k:     Fractional variance explained:\")\n",
    "    for k in K:\n",
    "        pca = PCA(n_components=k)\n",
    "        pca.fit(training_data)\n",
    "        print(\"%2s %18s\" % (k, np.sum(pca.explained_variance_ratio_)))\n",
    "### STUDENT END ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      " 1      0.76538335544\n",
      " 2     0.948983856061\n",
      " 3     0.983348751618\n",
      " 4     0.998704657852\n",
      " 5     0.999708153699\n",
      " 6     0.999891486841\n",
      " 7     0.999949681623\n",
      " 8     0.999999429716\n",
      " 9                1.0\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(train_numeric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "train_binary_data = binarize(train_numeric_data)\n",
    "dev_binary_data = binarize(dev_numeric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      " 1     0.439886723617\n",
      " 2     0.603598868173\n",
      " 3     0.763970460573\n",
      " 4     0.877066855752\n",
      " 5     0.931344676798\n",
      " 6     0.961546633226\n",
      " 7     0.987182746176\n",
      " 8     0.996811351306\n",
      " 9                1.0\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(train_binary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "def perform_GMM(train_data, train_labels, test_data, test_labels, p_components = np.arange(1, 5, 1)):\n",
    "### STUDENT START ###\n",
    "    g_components = np.arange(1, 4, 1)\n",
    "    covariance = [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_roc_auc = 0\n",
    "    best_accurate_gmm = None\n",
    "    best_roc_auc_gmm = None\n",
    "    for pcom in p_components:\n",
    "        pca = PCA(n_components=pcom)\n",
    "        train_transform = pca.fit_transform(train_data)\n",
    "        test_transform = pca.transform(test_data)\n",
    "        for gcom, cov in [(gcom,cov) for gcom in g_components for cov in covariance]:\n",
    "            gmm = GMM(n_components=gcom, covariance_type=cov)\n",
    "            gmm.fit(train_transform)\n",
    "            \n",
    "            test_preds = gmm.predict(test_transform)\n",
    "            \n",
    "            accuracy = metrics.accuracy_score(test_labels, gmm.predict(test_transform))\n",
    "            \n",
    "            roc_auc = metrics.roc_auc_score(test_labels, gmm.predict(test_transform))\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_accurate_gmm = gmm\n",
    "                ac_roc_auc = roc_auc\n",
    "            if roc_auc > best_roc_auc:\n",
    "                best_roc_auc = roc_auc\n",
    "                best_roc_auc_gmm = gmm\n",
    "                ra_acc = accuracy\n",
    "                \n",
    "\n",
    "    print(\"Best Test accuracy: %s\\n\" % (best_accuracy))\n",
    "    print(\"roc_auc of gmm classifier: %s\\n\" % (ac_roc_auc))\n",
    "    print(\"Best accurate gmm classifier: \\n%s\\n\" % (best_accurate_gmm))\n",
    "\n",
    "    \n",
    "    print(\"Best Test roc_auc: %s\\n\" % (best_roc_auc))\n",
    "    print(\"Accuracy of gmm classifier: %s\\n\" % (ra_acc))\n",
    "    print(\"Best accurate gmm classifier: \\n%s\\n\" % (best_roc_auc_gmm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.59190188172\n",
      "\n",
      "Accuracy of gmm classifier: 0.352\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='full', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(train_numeric_data, train_labels, dev_numeric_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.59765625\n",
      "\n",
      "Accuracy of gmm classifier: 0.356\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(train_binary_data, train_labels, dev_binary_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gmm_PosNeg(train_data, train_labels, test_data, test_labels, p_components = np.arange(1, 5, 1)):\n",
    "    train_data, test_data = np.asarray(train_data), np.asarray(test_data)\n",
    "    train_labels, test_labels = np.asarray(train_labels), np.asarray(test_labels) \n",
    "    g_components = np.arange(1, 4, 1)\n",
    "    covariance = [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_gmm = None\n",
    "    best_roc_auc = 0\n",
    "    for pcom in p_components:\n",
    "        pca = PCA(n_components=pcom)\n",
    "        train_transform = pca.fit_transform(train_data)\n",
    "        test_transform = pca.transform(test_data)\n",
    "        \n",
    "        positive_train = train_transform[np.where(train_labels==True)] \n",
    "        negative_train = train_transform[np.where(train_labels==False)]\n",
    "        \n",
    "        for gcom, cov in [(gcom,cov) for gcom in g_components for cov in covariance]:\n",
    "            gmm_positive = GMM(n_components=gcom, covariance_type=cov)\n",
    "            gmm_positive.fit(positive_train)\n",
    "\n",
    "            gmm_negative = GMM(n_components=gcom, covariance_type=cov)\n",
    "            gmm_negative.fit(negative_train)\n",
    "            \n",
    "            pos_scores = gmm_positive.score(test_transform)\n",
    "            neg_scores = gmm_negative.score(test_transform)\n",
    "            \n",
    "            pred_labels = [1 if pos_scores[i] > neg_scores[i] else 0 for (i, d) in enumerate(test_transform)]\n",
    "            pred_labels = np.asarray(pred_labels)\n",
    "            accuracy = np.mean(pred_labels.ravel() == test_labels.ravel()) * 100\n",
    "            \n",
    "            roc_auc = metrics.roc_auc_score(test_labels, pred_labels)\n",
    "            \n",
    "            roc_auc_pos = metrics.roc_auc_score(test_labels, gmm_positive.predict(test_transform))\n",
    "            roc_auc_neg = metrics.roc_auc_score(test_labels, gmm_negative.predict(test_transform))\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                acc_roc = roc_auc\n",
    "                acc_roc_pos = roc_auc_pos\n",
    "                acc_roc_neg = roc_auc_neg\n",
    "                best_gmm = gmm_positive\n",
    "            \n",
    "            if  roc_auc > best_roc_auc:\n",
    "                best_roc_auc = roc_auc\n",
    "                roc_accuracy = accuracy\n",
    "                roc_accuracy_pos = roc_auc_pos\n",
    "                roc_accuracy_neg = roc_auc_neg                \n",
    "                roc_gmm = gmm_positive\n",
    "\n",
    "    print(\"Best Test accuracy: %s\\n\" % (best_accuracy))\n",
    "    print(\"gmm classifier: \\n%s\" % (best_gmm))    \n",
    "    print(\"roc auc: \\n%s\\n\" % (acc_roc))\n",
    "    print(\"pos roc auc: \\n%s\\n\" % (acc_roc_pos))    \n",
    "    print(\"neg roc auc: \\n%s\\n\" % (acc_roc_neg))\n",
    "    \n",
    "    print(\"Best roc auc: \\n%s\" % (best_roc_auc))\n",
    "    print(\"gmm classifier: \\n%s\" % (roc_gmm))      \n",
    "    print(\"Accuracy: %s\\n\" % (roc_accuracy))  \n",
    "    print(\"pos roc auc: %s\\n\" % (roc_accuracy_pos))  \n",
    "    print(\"neg roc auc: %s\\n\" % (roc_accuracy_neg))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 70.6\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=2, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.525705645161\n",
      "\n",
      "pos roc auc: \n",
      "0.49470766129\n",
      "\n",
      "neg roc auc: \n",
      "0.495925739247\n",
      "\n",
      "Best roc auc: \n",
      "0.571824596774\n",
      "gmm classifier: \n",
      "GMM(covariance_type='full', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 58.4\n",
      "\n",
      "pos Accuracy: 0.558635752688\n",
      "\n",
      "neg Accuracy: 0.532888104839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(train_numeric_data, train_labels, dev_numeric_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 70.2\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.57169858871\n",
      "\n",
      "pos roc auc: \n",
      "0.5\n",
      "\n",
      "neg roc auc: \n",
      "0.5\n",
      "\n",
      "Best roc auc: \n",
      "0.686071908602\n",
      "gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 63.2\n",
      "\n",
      "pos Accuracy: 0.5\n",
      "\n",
      "neg Accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(train_binary_data, train_labels, dev_binary_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4671L, 13669L)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "print(tdf.shape)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 58.8\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.597572244624\n",
      "\n",
      "pos roc auc: \n",
      "0.5\n",
      "\n",
      "neg roc auc: \n",
      "0.5\n",
      "\n",
      "Best roc auc: \n",
      "0.605132728495\n",
      "gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 58.4\n",
      "\n",
      "pos Accuracy: 0.5\n",
      "\n",
      "neg Accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(tdf, train_labels, ddf, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.601709509409\n",
      "\n",
      "Accuracy of gmm classifier: 0.462\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(tdf, train_labels, ddf, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdf_binary = binarize(tdf)\n",
    "ddf_binary = binarize(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 64.4\n",
      "\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "roc auc: \n",
      "0.614709341398\n",
      "\n",
      "pos roc auc: \n",
      "0.5\n",
      "\n",
      "neg roc auc: \n",
      "0.5\n",
      "\n",
      "Best roc auc: \n",
      "0.614709341398\n",
      "gmm classifier: \n",
      "GMM(covariance_type='tied', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "Accuracy: 64.4\n",
      "\n",
      "pos Accuracy: 0.5\n",
      "\n",
      "neg Accuracy: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gmm_PosNeg(tdf_binary, train_labels, ddf_binary, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.632938508065\n",
      "\n",
      "Accuracy of gmm classifier: 0.31\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(tdf_binary, train_labels, ddf_binary, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "100     0.250625555279\n",
      "200     0.366817815376\n",
      "300     0.449003138972\n",
      "400     0.512314000251\n",
      "500     0.563696355339\n",
      "600     0.606643021743\n",
      "700     0.643378817516\n",
      "800     0.675381817972\n",
      "900     0.703665974325\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf, K=np.arange(100, 1000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "1000      0.72894345897\n",
      "1200     0.772477338559\n",
      "1400     0.808713532768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyboardInterrupt in 'zmq.backend.cython.message.Frame.__dealloc__' ignored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-ab08d940234c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperform_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-136-3a6a0c52ee81>\u001b[0m in \u001b[0;36mperform_PCA\u001b[1;34m(training_data, K)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%2s %18s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m### STUDENT END ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mexplained_variance_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         explained_variance_ratio_ = (explained_variance_ /\n\u001b[0;32m    277\u001b[0m                                      explained_variance_.sum())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf, K=np.arange(1000, 1400, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "200     0.567600356131\n",
      "400     0.722892908662\n",
      "600     0.804466683698\n",
      "800     0.855073456175\n",
      "1000     0.889495778778\n",
      "1200      0.91425701478\n",
      "1400     0.932798529542\n",
      "1600     0.947055924804\n",
      "1800      0.95822866246\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf_binary, K=np.arange(200, 2000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 13669L)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', norm='l1')\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "tdf_binary = binarize(tdf)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()\n",
    "ddf_binary = binarize(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "1400       0.9092050051\n",
      "1600     0.927980371388\n",
      "1800     0.942889553312\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(tdf_binary, K=np.arange(1400, 2000, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test accuracy: 0.744\n",
      "\n",
      "roc_auc of gmm classifier: 0.5\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=1, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n",
      "Best Test roc_auc: 0.60294858871\n",
      "\n",
      "Accuracy of gmm classifier: 0.58\n",
      "\n",
      "Best accurate gmm classifier: \n",
      "GMM(covariance_type='spherical', init_params='wmc', min_covar=0.001,\n",
      "  n_components=3, n_init=1, n_iter=100, params='wmc', random_state=None,\n",
      "  thresh=None, tol=0.001, verbose=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perform_GMM(tdf_binary, train_labels, ddf_binary, dev_labels, p_components=np.arange(1, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742\n",
      "0.514028897849\n",
      "0.824234639264\n",
      "0.645736307285\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=100)\n",
    "tdf_pca = pca.fit_transform(tdf_binary)\n",
    "ddf_pca = pca.transform(ddf_binary)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(tdf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(ddf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(ddf_pca))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(tdf_pca)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(tdf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744 0.5\n",
      "0.75294369514 0.5\n"
     ]
    }
   ],
   "source": [
    "gmm = GMM()\n",
    "\n",
    "gmm.fit(tdf_binary)\n",
    "\n",
    "dev_accuracy = metrics.accuracy_score(dev_labels, gmm.predict(ddf_binary))\n",
    "dev_roc_auc = metrics.roc_auc_score(dev_labels, gmm.predict(ddf_binary))\n",
    "\n",
    "train_accuracy = metrics.accuracy_score(train_labels, gmm.predict(tdf_binary))\n",
    "train_roc_auc = metrics.roc_auc_score(train_labels, gmm.predict(tdf_binary))\n",
    "\n",
    "print(dev_accuracy, dev_roc_auc)\n",
    "print(train_accuracy, train_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744\n",
      "0.512810819892\n",
      "0.788696210662\n",
      "0.573230345896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(tdf_binary, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(ddf_binary)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(ddf_binary))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(tdf_binary)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(tdf_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "tdf_pca = pca.fit_transform(tdf_binary)\n",
    "ddf_pca = pca.transform(ddf_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "train_binary_data = binarize(train_numeric_data)\n",
    "\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_binary_data = binarize(dev_numeric_data)\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.527091733871\n",
      "0.75594091201\n",
      "0.519456869309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_binary_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(dev_binary_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(dev_binary_data))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(train_binary_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(train_binary_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 100L)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 9L)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm='l2')\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "tdf_binary = binarize(tdf)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()\n",
    "ddf_binary = binarize(ddf)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "tdf_pca = pca.fit_transform(tdf_binary)\n",
    "ddf_pca = pca.transform(ddf_binary)\n",
    "\n",
    "train_combo = np.concatenate((tdf_pca, train_numeric_data), axis=1)\n",
    "dev_combo = np.concatenate((ddf_pca, dev_numeric_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_combo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3c90a7fa8f42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_combo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_combo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_combo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the best score we got all day\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_combo' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(train_combo, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gb.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gb.predict(dev_combo))) # the best score we got all day\n",
    "\n",
    "print(metrics.accuracy_score(train_labels, gb.predict(train_combo)))\n",
    "print(metrics.roc_auc_score(train_labels, gb.predict(train_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804973596647\n",
      "{'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.005, 0.01, 0.02],\n",
    "    'max_depth': [2, 3, 4],\n",
    "}\n",
    "\n",
    "gmm = GradientBoostingClassifier(random_state=0)\n",
    "gsc = GridSearchCV(gmm, param_grid, cv=6, verbose=0, scoring='roc_auc')\n",
    "gsc.fit(train_combo, train_labels)\n",
    "\n",
    "print (gsc.best_score_)\n",
    "print (gsc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n",
      "0.623865927419\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(dev_labels, gsc.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gsc.predict(dev_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782\n",
      "0.645959341398\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n",
      "0.513902889785\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(tdf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(ddf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(ddf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selected numeric columns from train_data and dev_data\n",
    "# number represents the column index starting at 0\n",
    "# 2 \t number_of_downvotes_of_request_at_retrieval\n",
    "# 6 \t request_number_of_comments_at_retrieval\n",
    "# 10\t requester_account_age_in_days_at_request\n",
    "# 12\t requester_days_since_first_post_on_raop_at_request\n",
    "# 14\t requester_number_of_comments_at_request\n",
    "# 16\t requester_number_of_comments_in_raop_at_request\n",
    "# 18\t requester_number_of_posts_at_request\n",
    "# 20\t requester_number_of_posts_on_raop_at_request\n",
    "# 22\t requester_number_of_subreddits_at_request\n",
    "\n",
    "train_numeric_data = train_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 6, 10, 12, 14, 16, 18, 20, 22]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n",
      "0.610803091398\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.521967405914\n"
     ]
    }
   ],
   "source": [
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_binary_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_binary_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_binary_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758\n",
      "0.573462701613\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((tdf_binary, train_binary_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((ddf_binary, dev_binary_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# selected numeric columns from train_data and dev_data\n",
    "# number represents the column index starting at 0\n",
    "# 2 \t number_of_downvotes_of_request_at_retrieval\n",
    "# 6 \t request_number_of_comments_at_retrieval\n",
    "# 10\t requester_account_age_in_days_at_request\n",
    "# 12\t requester_days_since_first_post_on_raop_at_request\n",
    "# 14\t requester_number_of_comments_at_request\n",
    "# 16\t requester_number_of_comments_in_raop_at_request\n",
    "# 18\t requester_number_of_posts_at_request\n",
    "# 20\t requester_number_of_posts_on_raop_at_request\n",
    "# 22\t requester_number_of_subreddits_at_request\n",
    "train_numeric_data = train_data[:, [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n",
      "0.712239583333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_numeric_data, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746\n",
      "0.578209005376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "train_binary = binarize(train_numeric_data)\n",
    "dev_binary = binarize(dev_numeric_data)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_binary, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_binary)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      " 1     0.999999994097\n",
      " 2     0.999999999561\n",
      " 3     0.999999999939\n",
      " 4     0.999999999991\n",
      " 5     0.999999999997\n",
      " 6     0.999999999999\n",
      " 7     0.999999999999\n",
      " 8                1.0\n",
      " 9                1.0\n",
      "10                1.0\n",
      "11                1.0\n",
      "12                1.0\n",
      "13                1.0\n",
      "14                1.0\n",
      "15                1.0\n",
      "16                1.0\n",
      "17                1.0\n",
      "18                1.0\n",
      "19                1.0\n"
     ]
    }
   ],
   "source": [
    "perform_PCA(train_numeric_data, K=np.arange(1, 20, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.714\n",
      "0.510584677419\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "train_num_pca = pca.fit_transform(train_numeric_data)\n",
    "dev_num_pca = pca.transform(dev_numeric_data)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_num_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_num_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_num_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n",
      "0.710769489247\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((tdf_pca, train_numeric_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((ddf_pca, dev_numeric_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.724336357527\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((tdf_binary, train_numeric_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((ddf_binary, dev_numeric_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n",
      "0.519027217742\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_title, dev_title = train_data[:, 9], dev_data[:, 9]\n",
    "train_titf, dev_titf = vectorizer.fit_transform(train_title).toarray(), vectorizer.transform(dev_title).toarray()\n",
    "\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_titf, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_titf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_titf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n",
      "0.99609375\n"
     ]
    }
   ],
   "source": [
    "dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "test_data, test_labels = np.delete(pizza_df[500:1000], 23, axis=1), [x for x in pizza_df[500:1000, 23]]\n",
    "train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# train_flair, dev_flair = train_data[:, 28], dev_data[:, 28]\n",
    "\n",
    "\n",
    "train_flair = [str(x) for x in train_data[:, 28]]\n",
    "dev_flair = [str(x) for x in dev_data[:, 28]]\n",
    "\n",
    "\n",
    "train_flaf, dev_flaf = vectorizer.fit_transform(train_flair).toarray(), vectorizer.transform(dev_flair).toarray()\n",
    "\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_flaf, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_flaf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_flaf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n",
      "0.511214717742\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# print(' '.join(train_data[1, 23]))\n",
    "\n",
    "# train_subreddits, dev_subreddits = train_data[:, 23], dev_data[:, 23]\n",
    "\n",
    "train_subreddits = [str(x) for x in train_data[:, 23]]\n",
    "\n",
    "dev_subreddits = [str(x) for x in dev_data[:, 23]]\n",
    "\n",
    "\n",
    "train_subf, dev_subf = vectorizer.fit_transform(train_subreddits).toarray(), vectorizer.transform(dev_subreddits).toarray()\n",
    "\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_subf, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_subf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_subf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:     Fractional variance explained:\n",
      "100     0.323573550518\n",
      "200     0.455603755206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-f914f206283a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperform_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_titf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-136-3a6a0c52ee81>\u001b[0m in \u001b[0;36mperform_PCA\u001b[1;34m(training_data, K)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%2s %18s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m### STUDENT END ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mexplained_variance_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         explained_variance_ratio_ = (explained_variance_ /\n\u001b[0;32m    277\u001b[0m                                      explained_variance_.sum())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perform_PCA(train_titf, K=np.arange(100, 1000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728\n",
      "0.509744623656\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=700)\n",
    "train_sub_pca, dev_sub_pca = pca.fit_transform(train_subf), pca.transform(dev_subf)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_sub_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_sub_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_sub_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.502310147849\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=700)\n",
    "train_titf_pca, dev_titf_pca = pca.fit_transform(train_titf), pca.transform(dev_titf)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_titf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_titf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_titf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n",
      "0.506090389785\n"
     ]
    }
   ],
   "source": [
    "train_subtitf_pca = np.concatenate((train_titf_pca, train_sub_pca), axis=1)\n",
    "dev_subtitf_pca = np.concatenate((dev_titf_pca, dev_sub_pca), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_subtitf_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_subtitf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_subtitf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.736\n",
      "0.525369623656\n"
     ]
    }
   ],
   "source": [
    "train_all_pca = np.concatenate((train_subtitf_pca, tdf_pca), axis=1)\n",
    "dev_all_pca = np.concatenate((dev_subtitf_pca, ddf_pca), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_all_pca, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_all_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_all_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.818\n",
      "0.716271841398\n"
     ]
    }
   ],
   "source": [
    "train_combo_bin = np.concatenate((train_all_pca, train_numeric_data), axis=1)\n",
    "dev_combo_bin = np.concatenate((dev_all_pca, dev_numeric_data), axis=1)\n",
    "gmm = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=2, random_state=0)\n",
    "gmm.fit(train_combo_bin, train_labels)\n",
    "print(metrics.accuracy_score(dev_labels, gmm.predict(dev_combo_bin)))\n",
    "print(metrics.roc_auc_score(dev_labels, gmm.predict(dev_combo_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822\n",
      "0.721522177419\n",
      "0.897024191822\n",
      "0.820123130583\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_numeric_data, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_numeric_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746\n",
      "0.516717069892\n",
      "0.798544208949\n",
      "0.592287694974\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', norm='l1')\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 7]).toarray()\n",
    "tdf_binary = binarize(tdf)\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 7]).toarray()\n",
    "ddf_binary = binarize(ddf)\n",
    "\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(tdf, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(ddf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(ddf)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(tdf)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(tdf)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746\n",
      "0.516717069892\n",
      "0.794262470563\n",
      "0.586242164205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(tdf_binary, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(ddf_binary)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(ddf_binary)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(tdf_binary)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(tdf_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.734\n",
      "0.493279569892\n",
      "0.820381074716\n",
      "0.63677291137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1400)\n",
    "tdf_pca = pca.fit_transform(tdf)\n",
    "ddf_pca = pca.transform(ddf)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(tdf_pca, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(ddf_pca)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(ddf_pca)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(tdf_pca)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(tdf_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.748\n",
      "0.574428763441\n",
      "29.0\n",
      "0.764076214943\n",
      "0.591232040069\n",
      "29.0\n"
     ]
    }
   ],
   "source": [
    "train_binary = binarize(train_numeric_data)\n",
    "dev_binary = binarize(dev_numeric_data)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_binary, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_binary)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_binary)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_binary)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_binary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836\n",
      "0.723244287634\n",
      "0.909869406979\n",
      "0.841461921275\n"
     ]
    }
   ],
   "source": [
    "train_combo = np.concatenate((tdf, train_numeric_data), axis=1)\n",
    "dev_combo = np.concatenate((ddf, dev_numeric_data), axis=1)\n",
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_combo, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_combo)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_combo)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 120 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   31.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:  3.2min\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  8.2min\n",
      "[Parallel(n_jobs=1)]: Done 720 out of 720 | elapsed: 13.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89839858955\n",
      "{'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 2}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 700, 1000],\n",
    "    'learning_rate': [0.005, 0.01, 0.02, 0.05, 0.07, 0.10],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "gbm = xgb.XGBClassifier()\n",
    "gsc = GridSearchCV(gbm, param_grid, cv=6, verbose=1, scoring='roc_auc')\n",
    "gsc.fit(train_numeric_data, train_labels)\n",
    "\n",
    "print (gsc.best_score_)\n",
    "print (gsc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826\n",
      "0.742145497312\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_numeric_data, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.734585013441\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_combo = np.concatenate((tdf, train_numeric_data), axis=1)\n",
    "dev_combo = np.concatenate((ddf, dev_numeric_data), axis=1)\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_combo, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_combo)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832\n",
      "0.720556115591\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_combo = np.concatenate((tdf_pca, train_numeric_data), axis=1)\n",
    "dev_combo = np.concatenate((ddf_pca, dev_numeric_data), axis=1)\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_combo, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_combo)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n",
      "0.704427083333\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "                          min_samples_split=2, random_state=0, \n",
    "                          criterion='entropy')\n",
    "clf4.fit(train_numeric_data, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, clf4.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, clf4.predict(dev_numeric_data)))\n",
    "print(metrics.accuracy_score(train_labels, clf4.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, clf4.predict(train_numeric_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier accuracy score: 0.826\n",
      "VotingClassifier roc_auc_score: 0.742145497312\n",
      "VotingClassifier accuracy score: 1.0\n",
      "VotingClassifier roc_auc_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# dev_data, dev_labels = np.delete(pizza_df[:500], 23, axis=1), [x for x in pizza_df[:500, 23]]\n",
    "# train_data, train_labels = np.delete(pizza_df[1000:], 23, axis=1), [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "# train_data_features = vectorizer.fit_transform(train_data[:, 7])\n",
    "# dev_data_features = vectorizer.transform(dev_data[:, 7])\n",
    "\n",
    "# clf1 = KNeighborsClassifier()\n",
    "# clf2 = BernoulliNB()\n",
    "# clf3 = LogisticRegression()\n",
    "# clf4 = ExtraTreesClassifier(n_estimators=250, max_depth=None, \n",
    "#                           min_samples_split=2, random_state=0, \n",
    "#                           criterion='entropy')\n",
    "clf5 = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "# clf6 = GradientBoostingClassifier(n_estimators=500, learning_rate=0.02, max_depth=2, random_state=0)\n",
    "\n",
    "vclf1 = VotingClassifier(estimators=[('xgb', clf5)], voting='hard')\n",
    "\n",
    "vclf1.fit(train_numeric_data, train_labels)\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(dev_labels, vclf1.predict(dev_numeric_data))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(dev_labels, vclf1.predict(dev_numeric_data))))\n",
    "\n",
    "print(\"VotingClassifier accuracy score: %s\" % (metrics.accuracy_score(train_labels, vclf1.predict(train_numeric_data))))\n",
    "print(\"VotingClassifier roc_auc_score: %s\" % (metrics.roc_auc_score(train_labels, vclf1.predict(train_numeric_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4671L,)\n",
      "(4671L, 21L)\n",
      "(500L,)\n",
      "(500L, 21L)\n"
     ]
    }
   ],
   "source": [
    "length_train_post = np.zeros(len(train_data[:, 7]))\n",
    "length_train_post.reshape(len(train_data[:, 7]), 1)\n",
    "for i, t in enumerate(train_data[:, 7]):\n",
    "    length_train_post[i] = len(t.split())\n",
    "print(length_train_post.shape)\n",
    "print(train_numeric_data.shape)\n",
    "\n",
    "length_dev_post = np.zeros(len(dev_data[:, 7]))\n",
    "length_dev_post.reshape(len(dev_data[:, 7]), 1)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    length_dev_post[i] = len(t.split())\n",
    "print(length_dev_post.shape)\n",
    "print(dev_numeric_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine the numeric data \n",
    "train_temp = []\n",
    "for i in range(train_numeric_data.shape[0]):\n",
    "    train_temp.append(np.hstack((train_numeric_data[i], length_train_post[i])))\n",
    "\n",
    "train_temp = np.asarray(train_temp)\n",
    "\n",
    "dev_temp = []\n",
    "for i in range(dev_numeric_data.shape[0]):\n",
    "    dev_temp.append(np.hstack((dev_numeric_data[i], length_dev_post[i])))\n",
    "dev_temp = np.asarray(dev_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 22L)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500L, 22L)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.742271505376\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_temp, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_temp)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary code reciprocity\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "reciprocity_terms = ['pay it forward', 'paying it forward', 'forward',  \n",
    "                     'reciprocate', 'give back', 'trade', 'pay it back'\n",
    "                    'pic', 'pics', 'story', 'stories', 'student', \n",
    "                     'pay', 'paycheck', 'job', 'money', 'work', 'college', \n",
    "                     'family', 'rent',\n",
    "                    'military', 'marine', 'army']\n",
    "\n",
    "def is_phrase_found(phrase_list, target_text):\n",
    "    found = False\n",
    "    for t in reciprocity_terms:\n",
    "        if t in target_text:\n",
    "            found = True\n",
    "            break\n",
    "    return found\n",
    "\n",
    "def count_phrase_found(phrase_list, target_text):\n",
    "    found = 0\n",
    "    for t in reciprocity_terms:\n",
    "        if t in target_text:\n",
    "            found += 1\n",
    "    return found\n",
    "\n",
    "reciproc_train_data = np.zeros(len(train_data[:, 7]))\n",
    "reciproc_train_data.reshape(len(train_data[:, 7]), 1)\n",
    "for i, t in enumerate(train_data[:, 7]):\n",
    "    reciproc_train_data[i] = int(count_phrase_found(reciprocity_terms, t))\n",
    "\n",
    "reciproc_dev_data = np.zeros(len(dev_data[:, 7]))\n",
    "reciproc_dev_data.reshape(len(dev_data[:, 7]), 1)\n",
    "for i, t in enumerate(dev_data[:, 7]):\n",
    "    reciproc_dev_data[i] = int(count_phrase_found(reciprocity_terms, t)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826\n",
      "0.737021169355\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Combine the numeric data \n",
    "train_tempr = []\n",
    "for i in range(train_temp.shape[0]):\n",
    "    train_tempr.append(np.hstack((train_temp[i], reciproc_train_data[i])))\n",
    "\n",
    "train_tempr = np.asarray(train_tempr)\n",
    "\n",
    "dev_tempr = []\n",
    "for i in range(dev_temp.shape[0]):\n",
    "    dev_tempr.append(np.hstack((dev_temp[i], reciproc_dev_data[i])))\n",
    "\n",
    "dev_tempr = np.asarray(dev_tempr)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_tempr, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_tempr)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_tempr)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_tempr)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_tempr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric_data = train_data[:, [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27]]\n",
    "train_labels = [x for x in pizza_df[1000:, 23]]\n",
    "\n",
    "dev_numeric_data = dev_data[:, [2, 3, 4, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27]]\n",
    "dev_labels = [x for x in pizza_df[:500, 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822\n",
      "0.721522177419\n",
      "0.897024191822\n",
      "0.820123130583\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_numeric_data, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_numeric_data)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_numeric_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.742271505376\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_temp = []\n",
    "for i in range(train_numeric_data.shape[0]):\n",
    "    train_temp.append(np.hstack((train_numeric_data[i], length_train_post[i])))\n",
    "\n",
    "train_temp = np.asarray(train_temp)\n",
    "\n",
    "dev_temp = []\n",
    "for i in range(dev_numeric_data.shape[0]):\n",
    "    dev_temp.append(np.hstack((dev_numeric_data[i], length_dev_post[i])))\n",
    "dev_temp = np.asarray(dev_temp)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_temp, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_temp)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836\n",
      "0.738617271505\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_combo = np.concatenate((tdf, train_tempr), axis=1)\n",
    "dev_combo = np.concatenate((ddf, dev_tempr), axis=1)\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_combo, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_combo)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_combo)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_combo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4671L, 23L)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tempr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-4fc5a9b61d5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\xgboost-0.4-py2.7.egg\\xgboost\\sklearn.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[0;32m    341\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                               verbose_eval=verbose)\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\xgboost-0.4-py2.7.egg\\xgboost\\training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\xgboost-0.4-py2.7.egg\\xgboost\\core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    695\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 8]).toarray()\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 8]).toarray()\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(tdf, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(ddf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(ddf)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(tdf)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(tdf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -16.62, time = 12.65s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -8.56, time = 12.85s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -6.78, time = 13.01s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -6.17, time = 13.47s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -5.85, time = 13.88s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -5.86, time = 13.29s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -5.64, time = 13.41s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -6.14, time = 13.45s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -5.83, time = 13.57s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -6.05, time = 13.91s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -5.64, time = 14.49s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -5.52, time = 13.74s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -5.90, time = 13.26s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -5.60, time = 13.83s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -5.33, time = 13.12s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -6.03, time = 13.23s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -5.54, time = 13.15s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -5.90, time = 13.07s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -5.31, time = 16.24s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -5.26, time = 15.35s\n",
      "()\n",
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.74      1.00      0.85       372\n",
      "       True       0.00      0.00      0.00       128\n",
      "\n",
      "avg / total       0.55      0.74      0.63       500\n",
      "\n",
      "\n",
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      0.81      0.78       372\n",
      "       True       0.27      0.20      0.23       128\n",
      "\n",
      "avg / total       0.63      0.66      0.64       500\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "tdf = vectorizer.fit_transform(train_data[:, 8]).toarray()\n",
    "\n",
    "ddf = vectorizer.transform(dev_data[:, 8]).toarray()\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000.0\n",
    "\n",
    "# Training RBM-Logistic Pipeline\n",
    "classifier.fit(tdf, train_labels)\n",
    "\n",
    "# Training Logistic regression\n",
    "logistic_classifier = LogisticRegression(C=100.0)\n",
    "logistic_classifier.fit(tdf, train_labels)\n",
    "\n",
    "###############################################################################\n",
    "# Evaluation\n",
    "\n",
    "print()\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        dev_labels,\n",
    "        classifier.predict(ddf))))\n",
    "\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        dev_labels,\n",
    "        logistic_classifier.predict(ddf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using RBM features:\n",
      "0.744\n",
      "\n",
      "Logistic regression using raw pixel features:\n",
      "0.508820564516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.accuracy_score(\n",
    "        dev_labels,\n",
    "        classifier.predict(ddf))))\n",
    "\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.roc_auc_score(\n",
    "        dev_labels,\n",
    "        logistic_classifier.predict(ddf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -190644527577.38, time = 0.11s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -414130065303.35, time = 0.22s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -637615604638.87, time = 0.19s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -861101144622.07, time = 0.17s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -1084586691704.46, time = 0.14s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -1308072218133.05, time = 0.15s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -1531557751021.56, time = 0.17s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -1755043299392.74, time = 0.14s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -1978528816784.66, time = 0.18s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -2202014380645.74, time = 0.15s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -2425499926437.65, time = 0.16s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -2648985464491.90, time = 0.14s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -2872470971551.98, time = 0.17s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -3095956556067.18, time = 0.15s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -3319442088950.97, time = 0.17s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -3542927570208.11, time = 0.14s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -3766413159881.74, time = 0.17s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -3989898661786.24, time = 0.14s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -4213384210159.02, time = 0.18s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -4436869794674.67, time = 0.16s\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000.0\n",
    "\n",
    "# Training RBM-Logistic Pipeline\n",
    "classifier.fit(train_tempr, train_labels)\n",
    "\n",
    "# Training Logistic regression\n",
    "logistic_classifier = LogisticRegression(C=100.0)\n",
    "logistic_classifier.fit(train_tempr, train_labels)\n",
    "\n",
    "###############################################################################\n",
    "# Evaluation\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.74      1.00      0.85       372\n",
      "       True       0.00      0.00      0.00       128\n",
      "\n",
      "avg / total       0.55      0.74      0.63       500\n",
      "\n",
      "\n",
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.74      1.00      0.85       372\n",
      "       True       0.00      0.00      0.00       128\n",
      "\n",
      "avg / total       0.55      0.74      0.63       500\n",
      "\n",
      "\n",
      "Logistic regression using RBM features:\n",
      "0.744\n",
      "\n",
      "Logistic regression using RBM features:\n",
      "0.5\n",
      "\n",
      "Logistic regression using raw features:\n",
      "0.744\n",
      "\n",
      "Logistic regression using raw features:\n",
      "0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        dev_labels,\n",
    "        classifier.predict(dev_tempr))))\n",
    "\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        dev_labels,\n",
    "        logistic_classifier.predict(dev_tempr))))\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.accuracy_score(\n",
    "        dev_labels,\n",
    "        classifier.predict(dev_tempr))))\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.roc_auc_score(\n",
    "        dev_labels,\n",
    "        classifier.predict(dev_tempr))))\n",
    "\n",
    "\n",
    "print(\"Logistic regression using raw features:\\n%s\\n\" % (\n",
    "    metrics.accuracy_score(\n",
    "        dev_labels,\n",
    "        logistic_classifier.predict(dev_tempr))))\n",
    "print(\"Logistic regression using raw features:\\n%s\\n\" % (\n",
    "    metrics.roc_auc_score(\n",
    "        dev_labels,\n",
    "        logistic_classifier.predict(dev_tempr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500L, 23L)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tempr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -19641563518830.93, time = 6.69s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -42134451981241.48, time = 10.64s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -64627340195680.59, time = 10.65s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -87120227749389.45, time = 10.61s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -109613116624932.61, time = 12.23s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -132106004921946.27, time = 13.69s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -154598893053969.47, time = 11.54s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -177091780194697.19, time = 10.82s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -199584668326545.66, time = 13.61s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -222077558606454.72, time = 10.92s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -244570447233988.56, time = 10.83s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -267063337183396.31, time = 11.76s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -289556223167457.44, time = 10.80s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -312049115429976.12, time = 10.79s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -334541998109657.94, time = 12.98s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -357034884424107.19, time = 12.10s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -379527773712571.69, time = 12.22s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -402020664322906.88, time = 11.67s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -424513555593996.56, time = 11.56s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -447006440256284.00, time = 11.22s\n",
      "Logistic regression using RBM features:\n",
      "0.744\n",
      "\n",
      "Logistic regression using RBM features:\n",
      "0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression()\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "classifier = Pipeline(steps=[('rbm', rbm), ('gbm', gbm)])\n",
    "\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 20\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 10000\n",
    "\n",
    "classifier.fit(train_tempr, train_labels)\n",
    "\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.accuracy_score(\n",
    "        dev_labels,\n",
    "        classifier.predict(dev_tempr))))\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.roc_auc_score(\n",
    "        dev_labels,\n",
    "        classifier.predict(dev_tempr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost regression using RBM features:\n",
      "0.75294369514\n",
      "\n",
      "XGBoost using RBM features:\n",
      "0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.accuracy_score(\n",
    "        train_labels,\n",
    "        classifier.predict(train_tempr))))\n",
    "print(\"XGBoost using RBM features:\\n%s\\n\" % (\n",
    "    metrics.roc_auc_score(\n",
    "        train_labels,\n",
    "        classifier.predict(train_tempr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "print (theano.config.device) # We're using CPUs (for now)\n",
    "print (theano.config.floatX) # Should be 64 bit for CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 21\n",
      "Train set = 4671\n",
      "Test set = 500\n"
     ]
    }
   ],
   "source": [
    "numFeatures = train_numeric_data[1].size\n",
    "numTrainExamples = train_numeric_data.shape[0]\n",
    "numTestExamples = dev_numeric_data.shape[0]\n",
    "print 'Features = %d' %(numFeatures)\n",
    "print 'Train set = %d' %(numTrainExamples)\n",
    "print 'Test set = %d' %(numTestExamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes = 2\n"
     ]
    }
   ],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size, 2))\n",
    "    for j in range(0,data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j,i]=1\n",
    "    return binarized_data\n",
    "\n",
    "train_labels = np.asarray(train_labels)\n",
    "dev_labels = np.asarray(dev_labels)\n",
    "\n",
    "train_labels_b = binarizeY(train_labels)\n",
    "dev_labels_b = binarizeY(dev_labels)\n",
    "numClasses = dev_labels_b[1].size\n",
    "print 'Classes = %d' %(numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (1) Parameters \n",
    "# Initialize the weights to small, but non-zero, values.\n",
    "w = theano.shared(np.asarray((np.random.randn(*(numFeatures, numClasses))*.01)))\n",
    "\n",
    "## (2) Model\n",
    "# Theano objects accessed with standard Python variables\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "y_hat = model(X, w)\n",
    "\n",
    "## (3) Cost function\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.7440\n",
      "1) roc_auc_score = 0.5000\n",
      "2) accuracy = 0.7440\n",
      "2) roc_auc_score = 0.5000\n",
      "3) accuracy = 0.7440\n",
      "3) roc_auc_score = 0.5000\n",
      "4) accuracy = 0.7440\n",
      "4) roc_auc_score = 0.5000\n",
      "5) accuracy = 0.7440\n",
      "5) roc_auc_score = 0.5000\n",
      "6) accuracy = 0.7440\n",
      "6) roc_auc_score = 0.5000\n",
      "7) accuracy = 0.7440\n",
      "7) roc_auc_score = 0.5000\n",
      "8) accuracy = 0.7440\n",
      "8) roc_auc_score = 0.5000\n",
      "9) accuracy = 0.7440\n",
      "9) roc_auc_score = 0.5000\n",
      "10) accuracy = 0.7440\n",
      "10) roc_auc_score = 0.5000\n",
      "11) accuracy = 0.7440\n",
      "11) roc_auc_score = 0.5000\n",
      "12) accuracy = 0.7440\n",
      "12) roc_auc_score = 0.5000\n",
      "13) accuracy = 0.7440\n",
      "13) roc_auc_score = 0.5000\n",
      "14) accuracy = 0.7440\n",
      "14) roc_auc_score = 0.5000\n",
      "15) accuracy = 0.7440\n",
      "15) roc_auc_score = 0.5000\n",
      "16) accuracy = 0.7440\n",
      "16) roc_auc_score = 0.5000\n",
      "17) accuracy = 0.7440\n",
      "17) roc_auc_score = 0.5000\n",
      "18) accuracy = 0.7440\n",
      "18) roc_auc_score = 0.5000\n",
      "19) accuracy = 0.7440\n",
      "19) roc_auc_score = 0.5000\n",
      "20) accuracy = 0.7440\n",
      "20) roc_auc_score = 0.5000\n",
      "21) accuracy = 0.7440\n",
      "21) roc_auc_score = 0.5000\n",
      "22) accuracy = 0.7440\n",
      "22) roc_auc_score = 0.5000\n",
      "23) accuracy = 0.7440\n",
      "23) roc_auc_score = 0.5000\n",
      "24) accuracy = 0.7440\n",
      "24) roc_auc_score = 0.5000\n",
      "25) accuracy = 0.7440\n",
      "25) roc_auc_score = 0.5000\n",
      "26) accuracy = 0.7440\n",
      "26) roc_auc_score = 0.5000\n",
      "27) accuracy = 0.7440\n",
      "27) roc_auc_score = 0.5000\n",
      "28) accuracy = 0.7440\n",
      "28) roc_auc_score = 0.5000\n",
      "29) accuracy = 0.7440\n",
      "29) roc_auc_score = 0.5000\n",
      "30) accuracy = 0.7440\n",
      "30) roc_auc_score = 0.5000\n",
      "31) accuracy = 0.7440\n",
      "31) roc_auc_score = 0.5000\n",
      "32) accuracy = 0.7440\n",
      "32) roc_auc_score = 0.5000\n",
      "33) accuracy = 0.7440\n",
      "33) roc_auc_score = 0.5000\n",
      "34) accuracy = 0.7440\n",
      "34) roc_auc_score = 0.5000\n",
      "35) accuracy = 0.7440\n",
      "35) roc_auc_score = 0.5000\n",
      "36) accuracy = 0.7440\n",
      "36) roc_auc_score = 0.5000\n",
      "37) accuracy = 0.7440\n",
      "37) roc_auc_score = 0.5000\n",
      "38) accuracy = 0.7440\n",
      "38) roc_auc_score = 0.5000\n",
      "39) accuracy = 0.7440\n",
      "39) roc_auc_score = 0.5000\n",
      "40) accuracy = 0.7440\n",
      "40) roc_auc_score = 0.5000\n",
      "41) accuracy = 0.7440\n",
      "41) roc_auc_score = 0.5000\n",
      "42) accuracy = 0.7440\n",
      "42) roc_auc_score = 0.5000\n",
      "43) accuracy = 0.7440\n",
      "43) roc_auc_score = 0.5000\n",
      "44) accuracy = 0.7440\n",
      "44) roc_auc_score = 0.5000\n",
      "45) accuracy = 0.7440\n",
      "45) roc_auc_score = 0.5000\n",
      "46) accuracy = 0.7440\n",
      "46) roc_auc_score = 0.5000\n",
      "47) accuracy = 0.7440\n",
      "47) roc_auc_score = 0.5000\n",
      "48) accuracy = 0.7440\n",
      "48) roc_auc_score = 0.5000\n",
      "49) accuracy = 0.7440\n",
      "49) roc_auc_score = 0.5000\n",
      "50) accuracy = 0.7440\n",
      "50) roc_auc_score = 0.5000\n",
      "train time = 0.47\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "## (4) Objective (and solver)\n",
    "\n",
    "alpha = 0.01\n",
    "gradient = T.grad(cost=cost, wrt=w) \n",
    "update = [[w, w - gradient * alpha]] \n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
    "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "def gradientDescent(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        cost = train(train_numeric_data[0:len(train_numeric_data)], train_labels_b[0:len(train_numeric_data)])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(dev_labels_b, axis=1) == predict(dev_numeric_data)))\n",
    "        print '%d) roc_auc_score = %.4f' %(i+1, metrics.roc_auc_score(dev_labels, predict(dev_numeric_data)))\n",
    "    print 'train time = %.2f' %(trainTime)\n",
    "\n",
    "gradientDescent(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(dev_numeric_data)   \n",
    "print 'predict time = %.2f' %(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) accuracy = 0.7440\n",
      "2) accuracy = 0.7440\n",
      "3) accuracy = 0.7440\n",
      "4) accuracy = 0.7440\n",
      "5) accuracy = 0.7440\n",
      "6) accuracy = 0.7440\n",
      "7) accuracy = 0.7440\n",
      "8) accuracy = 0.7440\n",
      "9) accuracy = 0.7440\n",
      "10) accuracy = 0.7440\n",
      "11) accuracy = 0.7440\n",
      "12) accuracy = 0.7440\n",
      "13) accuracy = 0.7440\n",
      "14) accuracy = 0.7440\n",
      "15) accuracy = 0.7440\n",
      "16) accuracy = 0.7440\n",
      "17) accuracy = 0.7440\n",
      "18) accuracy = 0.7440\n",
      "19) accuracy = 0.7440\n",
      "20) accuracy = 0.7440\n",
      "21) accuracy = 0.7440\n",
      "22) accuracy = 0.7440\n",
      "23) accuracy = 0.7440\n",
      "24) accuracy = 0.7440\n",
      "25) accuracy = 0.7440\n",
      "26) accuracy = 0.7440\n",
      "27) accuracy = 0.7440\n",
      "28) accuracy = 0.7440\n",
      "29) accuracy = 0.7440\n",
      "30) accuracy = 0.7440\n",
      "31) accuracy = 0.7440\n",
      "32) accuracy = 0.7440\n",
      "33) accuracy = 0.7440\n",
      "34) accuracy = 0.7440\n",
      "35) accuracy = 0.7440\n",
      "36) accuracy = 0.7440\n",
      "37) accuracy = 0.7440\n",
      "38) accuracy = 0.7440\n",
      "39) accuracy = 0.7440\n",
      "40) accuracy = 0.7440\n",
      "41) accuracy = 0.7440\n",
      "42) accuracy = 0.7440\n",
      "43) accuracy = 0.7440\n",
      "44) accuracy = 0.7440\n",
      "45) accuracy = 0.7440\n",
      "46) accuracy = 0.7440\n",
      "47) accuracy = 0.7440\n",
      "48) accuracy = 0.7440\n",
      "49) accuracy = 0.7440\n",
      "50) accuracy = 0.7440\n",
      "train time = 944.56\n",
      "predict time = 0.00\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "## (1) Parms\n",
    "numHiddenNodes = 600 \n",
    "w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, numClasses))*.01)))\n",
    "params = [w_1, w_2]\n",
    "\n",
    "\n",
    "## (2) Model\n",
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "srng = RandomStreams()\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        X *= srng.binomial(X.shape, p=1 - p)\n",
    "        X /= 1 - p\n",
    "    return X\n",
    "\n",
    "def model(X, w_1, w_2, p_1, p_2):\n",
    "    return T.nnet.softmax(T.dot(dropout(T.maximum(T.dot(dropout(X, p_1), w_1),0.), p_2), w_2))\n",
    "\n",
    "y_hat_train = model(X, w_1, w_2, 0.2, 0.5)\n",
    "y_hat_predict = model(X, w_1, w_2, 0., 0.)\n",
    "\n",
    "## (3) Cost...same as logistic regression\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(y_hat_train, Y))\n",
    "\n",
    "## (4) Minimization.  Update rule changes to backpropagation.\n",
    "alpha = 0.01\n",
    "def backprop(cost, w):\n",
    "    grads = T.grad(cost=cost, wrt=w)\n",
    "    updates = []\n",
    "    for w1, grad in zip(w, grads):\n",
    "        updates.append([w1, w1 - grad * alpha])\n",
    "    return updates\n",
    "update = backprop(cost, params)\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "y_pred = T.argmax(y_hat_predict, axis=1)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "miniBatchSize = 1\n",
    "def gradientDescentStochastic(epochs):\n",
    "    trainTime = 0.0\n",
    "    predictTime = 0.0\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(train_numeric_data), miniBatchSize), range(miniBatchSize, len(train_numeric_data), miniBatchSize)):\n",
    "            cost = train(train_numeric_data[start:end], train_labels_b[start:end])\n",
    "        trainTime =  trainTime + (time.time() - start_time)\n",
    "        print '%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(dev_labels_b, axis=1) == predict(dev_numeric_data)))\n",
    "    print 'train time = %.2f' %(trainTime)\n",
    "\n",
    "gradientDescentStochastic(50)\n",
    "\n",
    "start_time = time.time()\n",
    "predict(dev_numeric_data)   \n",
    "print 'predict time = %.2f' %(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n",
      "0.742271505376\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "train_temp = []\n",
    "for i in range(train_numeric_data.shape[0]):\n",
    "    train_temp.append(np.hstack((train_numeric_data[i], length_train_post[i])))\n",
    "\n",
    "train_temp = np.asarray(train_temp)\n",
    "\n",
    "dev_temp = []\n",
    "for i in range(dev_numeric_data.shape[0]):\n",
    "    dev_temp.append(np.hstack((dev_numeric_data[i], length_dev_post[i])))\n",
    "dev_temp = np.asarray(dev_temp)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_temp, train_labels)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.accuracy_score(train_labels, gbm.predict(train_temp)))\n",
    "print(metrics.roc_auc_score(train_labels, gbm.predict(train_temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.87671434,  0.889327  ,  0.85802802,  0.88655956,  0.91442688,\n",
       "        0.91146245,  0.87186265,  0.86906974,  0.87962344,  0.88765019])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(gbm, train_temp, train_labels, cv=10, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8525641 ,  0.84615385,  0.83119658,  0.84615385,  0.87794433,\n",
       "        0.8608137 ,  0.86723769,  0.83261803,  0.85622318,  0.87124464])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(gbm, train_temp, train_labels, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3fa8dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "predicted = cross_val_predict(gbm, dev_temp, dev_labels, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(dev_labels, predicted)\n",
    "ax.plot([dev_labels.min(), dev_labels.max()], [dev_labels.min(), dev_labels.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X98VPWd7/HXJ8EAoqAFoYCAGqysKFAXNV6UHXFb8K4/\ntuxWUa+uoCvC2mJZK3ShJV5pFa+tsUVctYBt166uax+PRavWak01Waig/IgoFahFRJctXvUqAUH4\n3D/mJA6ZSTI5ycyZM/N+Ph7zcM45X875JM7JZ74/zvdr7o6IiEhHlUUdgIiIxJMSiIiIhKIEIiIi\noSiBiIhIKEogIiISihKIiIiEEnkCMbOlZrbTzDa0cvxyM1sfvOrM7NR8xygiIukiTyDAcmBiG8f/\nAIx399HAQuCBvEQlIiJt6hZ1AO5eZ2bD2ji+KmVzFTA491GJiEh7CqEG0hHXAk9FHYSIiBRADSRb\nZnYuMBU4O+pYREQkJgnEzEYB9wOT3P39NsppYi8RkQ5ydwvz7wqlCcuCV/oBs6HAY8CV7r61vRO5\neyxfCxYsiDwGxR99HIo/nq84x98ZkddAzOznQALoa2ZvAQuACsDd/X7g28DngCVmZsB+dz8jqnhF\nRCQp8gTi7pe3c/zvgb/PUzgiIpKlQmnCKnmJRCLqEDpF8UdL8Ucr7vGHZZ1tAyskZubF9POIiOSa\nmeEx70QXEZGYUQIREZFQlEBERCQUJRAREQlFCUREREJRAhERkVCUQEREJBQlEBERCUUJREREQlEC\nERGRUJRAREQkFCUQEREJRQlERERCUQIREZFQlEBERCQUJRAREQlFCUREREJRAhERkVCUQEREJBQl\nEBERCUUJREREQok8gZjZUjPbaWYb2ijzQzPbbGbrzGxMPuMTEZHMIk8gwHJgYmsHzex8oNLdTwSm\nA/+cr8DyxcyaXyISL6V8/0aeQNy9Dni/jSIXAz8Nyv4O6GNmA/IRWz6YdQN6AicCPTGL/H+JiGSp\n1O/fOPy0g4HtKds7gn2xl/zGUgGsAt4I/tujJL/JiMSN7l/oFnUAXa26urr5fSKRIJFIRBZLdo4F\nRgXvR5HMjVuiC0dEOiB+929tbS21tbVdci5z9y45UaeCMBsGPO7uozIc+2fgeXd/JNjeBPyFu+/M\nUNYL4efJVvKbSk+S31xGARuAKmAPcfo5RErBnj176NHjsxpGsdy/Zoa7h6o2FUoTlgWvTFYAVwGY\nWRXwQabkEUfJD9lekh+6E4P/7o3Vh0+kFNTV1TF69GiWL1/evE/3bwHUQMzs50AC6AvsBBaQbFh0\nd78/KLMYmATsBqa6+yutnCtWNZAmqW2mcYxfpFg1NjYyb9487r77btyd3r178+qrrzJkyJDmMnG/\nfztTA4k8gXSluCYQESk8dXV1TJs2jc2bNx+yf+LEiTz11FNF01leDE1YIiIFw92ZNWtWWvIAOOGE\nE9i/f38EURUe1UBERDJYt24dp59+Op9++ikAw4YNY9myZUyYMCHiyLqWaiAiIl1szJgxzJ8/H4AZ\nM2bQ0NBQdMmjs1QDEZGSVl9fz8knn8zRRx+ddmz//v289NJLjBs3LoLI8kM1EBGRDmpsbOQb3/gG\n55xzDjfeeGPGMocddlhRJ4/OUg1EREpOphFWK1as4MILL4wwqmhoGG9ACURE2uLu/OM//iM1NTVp\nz2wMGjSIrVu30qNHj4iii4aasEREsmBm7N2b/rT4sGHD+NnPflZyyaOzVAMRkZLy8ccfc+qpp/LH\nP/4RSI6wWrRoEUceeWS0gUVENRARkSwdccQRLF26lOOPP57nnnuOJUuWlGzy6CzVQESk6DQ2NjJ/\n/ny++tWvctZZZ2Uss2/fPioqKvIcWeFRJ3pACUREUkdYnXTSSaxdu5aePXtGHVbBUhOWiJS8puc6\nxo8f3zw89/e//z0LFiyIOLLipRqIiMSeuzNu3DhWrlyZdqyyspINGzZw+OGHRxBZ4VMNRERKmpkx\na9astP0zZsxg7dq1Sh45ohqIiBQFd+erX/0qjz32WNHOnJsLnamBdOvqYEREcqmxsZHu3btTXl5+\nyH4zY8mSJRx33HEsWLBAQ3PzQE1YIhIbdXV1jBkzhrvuuivj8f79+3PnnXcqeeSJmrBEpOC1XJu8\ne/furFu3jhEjRkQdWuzpOZCAEohI8amrq2Pq1Kls2bLlkP1nnXUWL774YlpTlnSMRmGJSFFyd+bP\nn5+WPCC5YuC+ffsiiEqaqAYiIgVty5YtjBo1ij179gBw3HHHsXTpUo2w6iKqgYhI0Ro+fDi33XYb\nADNnztTa5AUk8hqImU0Cakgms6XuvqjF8d7AvwBDgXLg++7+YCvnUg1EJKbq6uqorKxk4MCBaccO\nHjzI6tWrOfPMMyOIrLjFtgZiZmXAYmAiMBK4zMxaDqv4B2Cju48BzgW+b2Z6fkWkSKTOYTV9+vS0\nxZ4AysrKlDwKUNRNWGcAm919m7vvBx4GLm5RxoGmQd1HAu+5+6d5jFFEcqSuro7Ro0c3LzH7+OOP\n89BDD0UdlmQp6gQyGNiesv12sC/VYuBkM3sHWA+kT3gjIrHzzW9+k/Hjx6eNsJo1axYfffRRRFFJ\nR8ShKWgisNbdJ5hZJfBrMxvl7h9nKlxdXd38PpFIkEgk8hKkiHRMRUVFWnNV0wgrPUmeO7W1tdTW\n1nbJuSLtRDezKqDa3ScF23MBT+1IN7MngNvcvT7Yfg6Y4+5rMpxPnegiMfHJJ5/w53/+52zcuBFI\njrBatGgRRxxxRMSRlZbYdqIDq4HhZjbMzCqAKcCKFmW2AX8JYGYDgC8Af8hrlCLS5bp3786DDz7I\n8OHDee6557jnnnuUPGKmUIbx3s1nw3hvN7PpJGsi95vZQOBBoGls323u/q+tnEs1EJEC0jSH1fnn\nn8+Xv/zljGU+/fRTunWLQ2t6cdJcWAElEJHCkTqH1ZAhQ2hoaKBPnz5RhyUtxLkJS0SKTOpzHU0j\nrLZv385NN90UcWTS1VQDEZEudd555/Gb3/wmbf/xxx/P2rVrVQspMKqBiEjBuPnmm9P2zZw5kw0b\nNih5FBklEBHpUhMnTuSaa64Bks91aIRV8dLQBxEJpbGxkfLycrp375527Pvf/z7HHHMM8+bNU+Io\nYuoDEZEOaxphdemll7Jw4cKow5FO0DDegBKISG61XJu8vLycVatWMXbs2KhDk5CUQAJKICK509ra\n5Keccgpr1qzJ2JQlhU+jsEQk5+68886Ma5OPHz+eAwcORBCRRE01EBHJyjvvvMPIkSP54IMPAK1N\nXixUAxGRnBs0aBA//OEPAa1NLkmqgYjIIerq6hg4cCCVlZVpx9yddevW8cUvfjGCyCQXVAMRkU5L\nncNq2rRpHDx4MK2MmSl5SDMlEBFJW5v8hRde4J577ok6LClwasISKXHf+ta3WLRoUdrysr169WLb\ntm307ds3osgkH9SEJSKh9e3bN+Pa5CtWrFDykDapBiJS4g4cOMDZZ5/NqlWrAK1NXmr0JHpACUQk\nnE2bNjF58mQWL16sobklRgkkoAQiklnTHFZnnXUWl1xyScYyBw8epKxMrdqlRgkkoAQiki51Dqu+\nffvy2muv0b9//6jDkgKhTnQRSZNpbfL33nuPmTNnpnWai4ShBaVEitSll17KE088kbb/5ZdfZteu\nXRxzzDERRCXFpM0mLDOb3dY/dvcfdHlEnaAmLJHPrFq1inHjxh3yRLlGWElLuWzCOjJ4jQVmAIOD\n1/XAaWEu2JKZTTKzTWb2hpnNaaVMwszWmtmrZvZ8V1xXpNhVVVUxe3byO6DWJpdcyKoT3cxeAP7K\n3T8Kto8Efunu4zt1cbMy4A3gPOAdYDUwxd03pZTpA/wn8GV332Fm/dx9VyvnUw1ESk5jYyPuTq9e\nvdKO7dmzh0WLFnHTTTcpcUhG+ehEHwDsS9neF+zrrDOAze6+zd33Aw8DF7cocznwmLvvAGgteYiU\noqY5rG6++eaMx3v27El1dbWSh+REtgnkp8BLZlZtZtXA74CfdMH1BwPbU7bfDval+gLwOTN73sxW\nm9mVXXBdkVhrOcJqyZIlPP+8Wnclv7IaheXu3zWzp4Bzgl1T3X1t7sI6RDeS/S0TgF7ASjNb6e7p\na2sC1dXVze8TiQSJRCIPIYrkT11dHdOmTWPz5s2H7J82bRoNDQ2qbUibamtrqa2t7ZJzdWQY7+HA\n/3P35WZ2jJkd7+5vdvL6O4ChKdvHBvtSvQ3scve9wN6gP2Y00G4CESlGDzzwQFryAJg0aVIE0Ujc\ntPxifcstt4Q+V7ad6AtIjsQ6yd2/YGaDgEfdfVzoKyfPWw78nmQn+rvAS8Bl7v56SpkRwI+ASUB3\nks1nl7r7axnOp050KXrvv/8+I0eO5N133wVg2LBhLF26lPPOOy/iyCSO8tGJ/hXgImA3gLu/Q3J4\nb6e4+wHgBuAZYCPwsLu/bmbTzey6oMwm4FfABmAVcH+m5CFSKo4++mjuu+8+AK6//noaGhqUPCQS\n2dZAXnL3M8zsFXc/zcx6ASvdfVTuQ8yeaiBSTOrr6+nduzennnpqxuMbN25k5MiReY5Kik0+aiD/\nZmb3AUeZ2d8DzwI/DnNBEWlbY2Mjs2fP5pxzzuGqq65i//79GcspeUjUsp6N18y+BHwZMOBX7v7r\nXAYWhmogEnf19fVMnTr1kE7yW265he985zsRRiXFLOfTuZvZInef096+qCmBSJwtWLCAW2+9NW2m\n3IqKCt58800GDRoUUWRSzPLRhPWlDPvOD3NBEcls6NChaclj2LBhPPnkk0oeUpDaTCBmNsPMGoAR\nZrYh5fUm0JCfEEVKw7Rp05g4cWLztkZYSaFrbzr3PsDRwG3A3JRDH7n7/81xbB2mJiyJu+3bt3PB\nBRfwgx/8QIlD8iIffSBVwMaU2Xh7A3/m7r8Lc9FcUQKRQtfY2Mj8+fMZMWIE1113XcYy7o5ZqPtZ\npMPykUDWAqc1/XUOpmFf4+5dsiZIV1ECkUKWOsLqiCOOoKGhgeOOOy7qsKTE5aMT/ZC/zO5+EC2H\nK5KV1Oc6mobnfvzxx1x77bVam1xiLdsE8gcz+7qZHRa8ZgF/yGVgIsVi2rRp3HXXXWnJYsuWLezY\n0XLuUJH4yDaBXA/8D5Iz5b4NnAlkbsAVkUPMnz+fioqKQ/Y1jbA69thjI4pKpPOyfhI9DtQHIoXq\ne9/7HvPmzdPMuVJwOtMH0mY/hpnd7O53mNmPgLS/zO7+9TAXFSlGjY2N7Nu3j6OOOirt2M0334yZ\nccMNN3DkkZ2eyFqkILT3HMiF7v64mf1dpuPu3hXL2nYZ1UAkKk0jrMaOHcvPf/7zqMMRyVrOh/HG\nhRKI5FvTcx01NTXNneS/+MUv+MpXvhJxZCLZyVkCMbPHydB01cTdLwpz0VxRApF8yjRzLkD//v15\n7bXX6Nu3b0SRiWQvZ30gwJ3BfycDnwf+Jdi+DNgZ5oIixeLRRx/NuDb55MmT00ZdiRSjbJ9EX+Pu\nY9vbFzXVQCSfdu/ezejRo9m6dSugtcklnvLxJHovMzsh5YLHA73CXFCkWPTq1Yvly5dTVlammXOl\nJGVbA5kE3E/y6XMDhgHT3f1XuQ2vY1QDkVyor6+nvLycqqqqjMe3bNnC8OHD8xyVSNfIyygsM+sO\njAg2N7n7J2EumEtKINKVUkdYVVZWsn79eg4//PCowxLpUjlvwjKzw4FvAje4+3pgqJldEOaCInFQ\nX1/PmDFjmuew2rJlC/PmzYs6LJGCkm0T1iPAy8BV7n5KkFD+093H5DrAjlANRLrCwoUL+c53vpM2\n+WFZWRlvvPEGlZWVEUUm0vXy0Yle6e53APsB3L2RZF9Ip5nZJDPbZGZvmNmcNsqdbmb7zWxyV1xX\npDUnn3xyxrXJn3nmGSUPkRTZJpB9ZtaT4KFCM6sEOt0HEixMtRiYCIwELjOzEa2Uux0oqE57KU6T\nJ09mypQpzdsaYSWSWbaLQi0AngaGmNlDwDjg6i64/hnAZnffBmBmDwMXA5talPsa8O/A6V1wTZF2\n/ehHP+LNN9/ku9/9rhKHSCvaTSCWXJx5E8mn0atINl3NcvddXXD9wcD2lO23SSaV1OsPAv7a3c81\ns0OOiYTVNMJqwIABzJmT3nLar18/Vq5cqbXJRdrQbgJxdzezJ939VOCXeYippRog9Q5v846urq5u\nfp9IJEgkEjkJSuIrdQ6riooKLrjgAkaOHJlWTslDilFtbS21tbVdcq5sR2H9BFjs7qu75KqfnbcK\nqHb3ScH2XJI5a1FKmaalcw3oB+wGrnP3FRnOp1FY0qpMM+cCjB07lpUrV9KtW7YtuiLFI5eTKTY5\nE/hfZvZHkn/AjeQf+lFhLppiNTDczIYB7wJTSE7U2MzdU6dQWQ48nil5iLTna1/7GsuWLUvb/6c/\n/Ym33nqLE044IcO/EpHWZDsKayJwAjABuBC4IPhvp7j7AeAG4BlgI/Cwu79uZtPNLNOa66peSGjf\n/va36dXr0CncmkZYKXmIdFx764H0AK4HhgMNwFJ3/zRPsXWYmrCkPffeey8zZ87UzLkigVw2Yf2E\n5MODLwLnAycDs8JcSCRfGhsb2b17N8ccc0zasenTp/PJJ59wzTXXaG1ykU5qrwbSEIy+wsy6AS+5\n+2n5Cq6jVAORphFWlZWVPPnkkxpJJdKOXE5lsr/pTSE3XYk0NjYye/ZszjnnHDZv3szTTz/N8uXL\now5LpKi1VwM5QHLUFSRHXvUEmubBcnfvnfMIO0A1kNLU2trkvXv35tVXX2XIkCERRSZS+HLWB+Lu\n5eFCEsmfZ599NuPa5JdffjlHHXVUBBGJlIasF5SKA9VAStO+ffs444wzWL9+PaC1yUU6Ih/TuYsU\nrIqKCh588EEOO+wwzZwrkkeqgUhs1NfXs3fv3laTw1tvvcXQoUPzHJVIvKkGIkUtdYTVlVdeyfvv\nv5+xnJKHSH4pgUhBa7k2+bvvvsuNN94YdVgigpqwpIDdcccdzJ07N215WYANGzZw6qmnRhCVSHFR\nE5YUpbFjx2Zcm/zZZ59V8hApAEogUrAmTJjAjBkzmrc1wkqksKgJSwqCu2ect+rjjz/mwgsvZP78\n+UocIjnQmSYsJRCJVNMqgRUVFdx+++1RhyNScpRAAkog8ZI6h1VZWRn19fVUVVVFHZZISVEnusRK\ny5lzAQ4ePMjVV1/Nnj17Io5ORLKlBCJ5N3fu3ObnOlLt3buXbdu2RRSViHSUmrAk73bu3MnIkSN5\n7733mvddf/313HHHHVolUCTP1IQlsTJgwADuuece4LPnOu69914lD5GYaW9NdJHQGhsbef/99xk8\neHDasUsuuYQPP/yQyy67TIlDJKbUhCU50TTCql+/frz44ouUl2ttMpFCpCYsKRgtR1itXLmSmpqa\nqMMSkRyIvAZiZpOAGpLJbKm7L2px/HJgTrD5ETDD3RtaOZdqIBFqbW3yHj16sG7dOk466aSIIhOR\n1sS2BmJmZcBiYCIwErjMzEa0KPYHYLy7jwYWAg/kN0rJ1ssvv5xxbfKrr76aQYMGRRCRiORSpDUQ\nM6sCFrj7+cH2XMBb1kJSyh8FNLj7kFaOqwYSoYMHD5JIJHjxxRcBrU0uEgexrYEAg4HtKdtvB/ta\ncy3wVE4jktDKyspYtmwZvXr10sy5IiUgNsN4zexcYCpwdlvlqqurm98nEgkSiURO4ypF9fX17Nq1\ni4svvjjt2PDhw9m8eTMDBw6MIDIRaU9tbS21tbVdcq5CaMKqdvdJwXbGJiwzGwU8Bkxy961tnE9N\nWDnUNHNuTU0Nffr04bXXXlOiEIm5ODdhrQaGm9kwM6sApgArUguY2VCSyePKtpKH5FbLtck/+OAD\npk+fnnG5WREpDYUyjPduPhvGe7uZTSdZE7nfzB4AJgPbAAP2u/sZrZxLNZAcqKmpYfbs2RmTxapV\nqzjzzDMjiEpEuoLWAwkogeTGmjVrqKqq4sCBA837NMJKpDjEuQlLYmDs2LHMmTOneVsjrEQEVAOR\nFlpbm/yTTz7hb//2b7nxxhuVOESKiJqwAkog4TU2NjJv3jwaGxu57777og5HRPJECSSgBBJOXV0d\n06ZNa56G5Omnn2bixIkRRyUi+aA+EAmlsbGRb3zjG4wfP/6QOayuvfZaPvzwwwgjE5E4UAIpYQsX\nLqSmpiZteG55eTlvvfVWRFGJSFyoCauEffjhh5xyyim8/fbbzftmzJjBokWLtEqgSIlQE5aE0qdP\nH3784x8Dyec6nnvuOZYsWaLkISJZUQ2kBDQ2NrJz506OP/74jMcfeughLrroIiUOkRKkUVgBJZB0\nTSOsunfvzpo1a+jevXvUIYlIAVETlqRpOcLq1Vdf5dZbb406LBEpIqqBFKHW1iYvLy/npZde4rTT\nTosoMhEpNKqByCG2bt2acW3y6667jhNPPDGCiESkGKkGUoTcnYsuuognnngCSI6wWrZsGRMmTIg4\nMhEpNOpEDyiBfOadd97hlFNOYcqUKXquQ0RapQQSKLUEUldXx7Zt27jiiisyHt+1axf9+vXLc1Qi\nEidKIIFSSSBNM+fefffd9OzZkw0bNlBZWRl1WCISQ0oggVJIIC1nzgUYP348zz//PGVlGhMhIh2j\nUVglYsmSJWkz5wK88MIL/Pa3v40oKhEpVUogMTJhwgQqKioO2dc0h9W5554bUVQiUqqUQGJkxIgR\nLFy4sHl7xowZNDQ0aHiuiERCfSAF6uDBgxn7NA4cOMAVV1zBddddp8QhIp2mTvRAMSSQphFWO3bs\n4JFHHsEs1P9XEZGsxLoT3cwmmdkmM3vDzOa0UuaHZrbZzNaZ2Zh8x5gvdXV1jBkzhpqaGh599FEe\nffTRqEMSEWlVpDUQMysD3gDOA94BVgNT3H1TSpnzgRvc/a/M7EzgbnevauV8sayBtFbL6NevHxs3\nbqR///55jkhEspV6/8b1709cayBnAJvdfZu77wceBi5uUeZi4KcA7v47oI+ZDchvmLlj1g3olvFY\nr1692LFjR34DEpGsJe/fnsCJQE+S34lLR9Q/7WBge8r228G+tsrsyFAmlpLfXCqAeuCkQ441jbD6\n4he/GEVoItKOz+7fVSQbUlYBPUqq3zLzV98Yq66ubn6fSCRIJBKRxZKdY0lWxJYDZ5PM6Z+yZMmS\nSKMSkWwcC4wK3o8i+d12S3ThZKG2tpba2touOVfUfSBVQLW7Twq25wLu7otSyvwz8Ly7PxJsbwL+\nwt13ZjhfrPpAkt9UepL85jIKuAv4J2BvLNtSRUpJ+v27AagC9sTq/u1MH0jUNZDVwHAzGwa8C0wB\nLmtRZgXwD8AjQcL5IFPyiCN3D9pMq0h+c9kBfBKrD59Iqcp8/5bWl79IE4i7HzCzG4BnSLbdLHX3\n181sevKw3+/uT5rZ/zSzLcBuYGqUMXc194PBN5ktwXbpfPhE4q7U7189SCgiUsLiPIxXRERiSglE\nRERCUQIREZFQlEBERCQUJRAREQlFCUREREJRAhERkVCUQEREJBQlEBERCUUJREREQlECERGRUJRA\nREQkFCUQEREJRQlERERCUQIREZFQlEBERCQUJRAREQlFCUREREJRAhERkVCUQEREJBQlEBERCUUJ\nREREQoksgZjZ0Wb2jJn93sx+ZWZ9MpQ51sx+Y2YbzazBzL4eRawiIpIuyhrIXOBZdz8J+A3wrQxl\nPgVmu/tI4CzgH8xsRB5jzJva2tqoQ+gUxR8txR+tuMcfVpQJ5GLgJ8H7nwB/3bKAu/+Xu68L3n8M\nvA4MzluEeRT3D6Dij5bij1bc4w8rygTS3913QjJRAP3bKmxmxwFjgN/lPDIREWlXt1ye3Mx+DQxI\n3QU4MD9DcW/jPEcA/w7MCmoiIiISMXNv9e92bi9s9jqQcPedZvZ54Hl3/7MM5boBTwBPufvd7Zwz\nmh9GRCTG3N3C/Luc1kDasQK4GlgE/B3wH62UWwa81l7ygPC/BBER6bgoayCfA/4NGAJsAy5x9w/M\nbCDwgLtfYGbjgBeABpJNXA78k7s/HUnQIiLSLLIEIiIi8RbbJ9Hj+iCimU0ys01m9oaZzWmlzA/N\nbLOZrTOzMfmOsS3txW9ml5vZ+uBVZ2anRhFna7L5/QflTjez/WY2OZ/xtSfLz0/CzNaa2atm9ny+\nY2xNFp+d3ma2IvjcN5jZ1RGE2SozW2pmO81sQxtlCvnebTP+UPeuu8fyRbLv5Obg/Rzg9gxlPg+M\nCd4fAfweGBFhzGXAFmAYcBiwrmU8wPnAL4P3ZwKrov5ddzD+KqBP8H5S3OJPKfccycEbk6OOu4O/\n/z7ARmBwsN0v6rg7EPu3gNua4gbeA7pFHXtKfGeTfJRgQyvHC/bezTL+Dt+7sa2BEM8HEc8ANrv7\nNnffDzxM8udIdTHwUwB3/x3Qx8wGUBjajd/dV7n7h8HmKgrrwc9sfv8AXyM5bPy/8xlcFrKJ/3Lg\nMXffAeDuu/IcY2uyid2BI4P3RwLvufuneYyxTe5eB7zfRpFCvnfbjT/MvRvnBBLHBxEHA9tTtt8m\n/X9SyzI7MpSJSjbxp7oWeCqnEXVMu/Gb2SDgr939XpLPLRWSbH7/XwA+Z2bPm9lqM7syb9G1LZvY\nFwMnm9k7wHpgVp5i6yqFfO92VFb3bpTDeNulBxHjy8zOBaaSrDbHSQ3JJtEmhZZE2tMNOA2YAPQC\nVprZSnffEm1YWZkIrHX3CWZWCfzazEbpns2vjty7BZ1A3P1LrR0LOoMG+GcPImZsbggeRPx34Gfu\n3tqzJvmyAxiasn1ssK9lmSHtlIlKNvFjZqOA+4FJ7t5WlT/fsol/LPCwmRnJdvjzzWy/u6/IU4xt\nySb+t4Fd7r4X2GtmLwCjSfY/RCmb2KcCtwG4+1YzexMYAazJS4SdV8j3blY6eu/GuQmr6UFE6KIH\nEfNgNTDczIaZWQUwheTPkWoFcBWAmVUBHzQ11RWAduM3s6HAY8CV7r41ghjb0m787n5C8Dqe5BeP\nmQWSPCC7z89/AGebWbmZHU6yM/f1PMeZSTaxbwP+EiDoO/gC8Ie8Rtk+o/VaaSHfu01ajT/UvRv1\nyIBOjCi7+3r5AAADAElEQVT4HPAsyZFVzwBHBfsHAk8E78cBB0iO+FgLvEIys0YZ96Qg5s3A3GDf\ndOC6lDKLSX5jXA+cFvXvuiPxAw+QHD3zSvA7fynqmDv6+08pu4wCGoXVgc/PTSRHYm0AvhZ1zB34\n7AwEfhXEvQG4LOqYW8T/c+Ad4BPgLZI1pjjdu23GH+be1YOEIiISSpybsEREJEJKICIiEooSiIiI\nhKIEIiIioSiBiIhIKEogIiISihKICGBmB83spynb5Wb2JzMrlIcIMwrmvDot6jikNCmBiCTtBk4x\ns+7B9pc4dGK8vDGz8iiuK9JRSiAin3kS+Kvg/WXAvzYdMLPDgwV5VpnZy2Z2YbB/mJm9YGZrgldV\nsP/zZvZbM3vFzDZYcnlmzOyjlHP+jZktD94vN7N7zWwVsCjD9S4KyvUws3+15CJpvwB65OH3IpJR\nQU+mKJJHTnKNigVm9ktgFLAUOCc4Pg94zt2vseTqly+Z2bPATuAv3X2fmQ0nmXROJ7kux9Puflsw\nMePhKddped0mg929KQF9N8P1fg1cD+x295HBinGvdOlvQaQDlEBEAu7+arBuzGXALzl00rkvAxea\n2TeD7QqSs8u+CywOli89AJwYHF8NLDWzw4D/cPf1WYTwaBbXGw/cHcTbYGbZnFckJ5RARA61Avg/\nQILkdO5NDPgbd9+cWtjMFgD/5e6jgr6LPQDu/qKZjSfZJPagmX3f3f+lxbVaNj/tbrGd6Xot443b\neiVSRNQHIpLU9Id4GXCLu29scfxXwNebCydrHJBcg/zd4P1VQHlwfCjw3+6+FPgxyUWeAP7LzE4y\nszLgK23E09r1XgCuCPadQrKpTSQSSiAiSQ7g7jvcfXGG47cChwUd4g3A/w72LwGuNrO1JNevaFo9\nLwGsN7NXgEsImp2Ab5FsHqsjObX2IddPsbCV690LHGFmG4Fq4rPYkhQhTecuIiKhqAYiIiKhKIGI\niEgoSiAiIhKKEoiIiISiBCIiIqEogYiISChKICIiEooSiIiIhPL/AX2zHQX0+WtMAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e3fada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "\n",
    "predicted = cross_val_predict(gbm, train_temp, train_labels, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train_labels, predicted)\n",
    "ax.plot([train_labels.min(), train_labels.max()], [train_labels.min(), train_labels.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81578947,  0.83198381,  0.77546778,  0.92307692,  0.85446985,\n",
       "        0.79417879,  0.82744283,  0.86902287,  0.83108108,  0.86261261])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(gbm, dev_temp, dev_labels, cv=10, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.76470588,  0.82352941,  0.8       ,  0.88      ,  0.8       ,\n",
       "        0.76      ,  0.84      ,  0.8       ,  0.73469388,  0.81632653])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(gbm, dev_temp, dev_labels, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True,  True, False, False,  True, False,\n",
       "       False, False,  True, False, False, False,  True, False,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "        True, False, False,  True, False, False, False,  True, False,\n",
       "       False,  True,  True, False, False,  True, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False,  True,  True, False, False,  True, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False,  True,\n",
       "       False,  True,  True, False, False,  True, False,  True, False,\n",
       "        True,  True, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True,  True,  True, False, False,\n",
       "        True, False, False, False, False, False,  True,  True,  True,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False,  True,  True, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True,  True,\n",
       "       False, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False,  True,  True, False,\n",
       "       False, False,  True, False, False, False, False,  True, False,\n",
       "        True, False, False, False, False, False, False, False,  True,\n",
       "        True, False, False, False, False, False, False,  True,  True,\n",
       "       False,  True, False, False, False,  True, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False,  True, False,  True,  True, False,\n",
       "        True, False, False,  True, False, False, False,  True, False,\n",
       "        True, False, False, False,  True,  True, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "        True,  True, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_predict(gbm, dev_temp, dev_labels, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154L, 21L)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_pos = train_numeric_data[np.where(train_labels==1)]\n",
    "train_labels_pos = train_labels[np.where(train_labels==1)]\n",
    "train_numeric_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3517L, 21L)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_neg = train_numeric_data[np.where(train_labels==0)]\n",
    "train_numeric_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_wanted = []\n",
    "train_labels_wanted = []\n",
    "\n",
    "for i in range(train_numeric_pos.shape[0]):\n",
    "    rand_index = random.randint(0, train_numeric_pos.shape[0])\n",
    "    train_wanted.append(train_numeric_data[int(rand_index)])\n",
    "    train_labels_wanted.append(train_labels[int(rand_index)])\n",
    "\n",
    "train_wanted = np.asarray(train_wanted)\n",
    "train_labels_wanted = np.asarray(train_labels_wanted)\n",
    "\n",
    "\n",
    "\n",
    "train_equal = np.concatenate((train_numeric_pos, train_wanted), axis=0)\n",
    "train_labels_equal = np.concatenate((train_labels_pos, train_labels_wanted), axis=0)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_equal, train_labels_equal)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_numeric_data)))\n",
    "print(metrics.accuracy_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "print(metrics.roc_auc_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='roc_auc'))\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n",
      "0.764112903226\n",
      "1.0\n",
      "1.0\n",
      "[ 0.96958784  0.97543017  0.94853942  0.96116035  0.94488316  0.92596217\n",
      "  0.93240378  0.94944553  0.96697652  0.98499674]\n",
      "[ 0.92241379  0.93965517  0.89224138  0.92207792  0.87012987  0.85217391\n",
      "  0.89130435  0.9         0.93913043  0.93913043]\n"
     ]
    }
   ],
   "source": [
    "train_numeric_pos = train_temp[np.where(train_labels==1)]\n",
    "train_labels_pos = train_labels[np.where(train_labels==1)]\n",
    "train_numeric_pos.shape\n",
    "train_numeric_neg = train_temp[np.where(train_labels==0)]\n",
    "train_numeric_neg.shape\n",
    "\n",
    "train_wanted = []\n",
    "train_labels_wanted = []\n",
    "\n",
    "for i in range(train_numeric_pos.shape[0]):\n",
    "    rand_index = random.randint(0, train_numeric_pos.shape[0])\n",
    "    train_wanted.append(train_temp[int(rand_index)])\n",
    "    train_labels_wanted.append(train_labels[int(rand_index)])\n",
    "\n",
    "train_wanted = np.asarray(train_wanted)\n",
    "train_labels_wanted = np.asarray(train_labels_wanted)\n",
    "\n",
    "train_equal = np.concatenate((train_numeric_pos, train_wanted), axis=0)\n",
    "train_labels_equal = np.concatenate((train_labels_pos, train_labels_wanted), axis=0)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_equal, train_labels_equal)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_temp)))\n",
    "print(metrics.accuracy_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "print(metrics.roc_auc_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='roc_auc'))\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.724\n",
      "0.768397177419\n",
      "1.0\n",
      "1.0\n",
      "[ 0.97682383  0.97985027  0.93540937  0.96351243  0.94017642  0.92040568\n",
      "  0.93322515  0.90580122  0.9873428   0.98166329]\n",
      "[ 0.92241379  0.92241379  0.85775862  0.91774892  0.86580087  0.85652174\n",
      "  0.88695652  0.87826087  0.96086957  0.94347826]\n"
     ]
    }
   ],
   "source": [
    "train_numeric_pos = train_tempr[np.where(train_labels==1)]\n",
    "train_labels_pos = train_labels[np.where(train_labels==1)]\n",
    "train_numeric_pos.shape\n",
    "train_numeric_neg = train_tempr[np.where(train_labels==0)]\n",
    "train_numeric_neg.shape\n",
    "\n",
    "train_wanted = []\n",
    "train_labels_wanted = []\n",
    "\n",
    "for i in range(train_numeric_pos.shape[0]):\n",
    "    rand_index = random.randint(0, train_numeric_pos.shape[0])\n",
    "    train_wanted.append(train_tempr[int(rand_index)])\n",
    "    train_labels_wanted.append(train_labels[int(rand_index)])\n",
    "\n",
    "train_wanted = np.asarray(train_wanted)\n",
    "train_labels_wanted = np.asarray(train_labels_wanted)\n",
    "\n",
    "train_equal = np.concatenate((train_numeric_pos, train_wanted), axis=0)\n",
    "train_labels_equal = np.concatenate((train_labels_pos, train_labels_wanted), axis=0)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_equal, train_labels_equal)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(dev_tempr)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(dev_tempr)))\n",
    "print(metrics.accuracy_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "print(metrics.roc_auc_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='roc_auc'))\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402\n",
      "0.562247983871\n",
      "0.989168110919\n",
      "0.985448195576\n"
     ]
    }
   ],
   "source": [
    "train_numeric_pos = tdf[np.where(train_labels==1)]\n",
    "train_labels_pos = train_labels[np.where(train_labels==1)]\n",
    "train_numeric_pos.shape\n",
    "train_numeric_neg = tdf[np.where(train_labels==0)]\n",
    "train_numeric_neg.shape\n",
    "\n",
    "train_wanted = []\n",
    "train_labels_wanted = []\n",
    "\n",
    "for i in range(train_numeric_pos.shape[0]):\n",
    "    rand_index = random.randint(0, train_numeric_pos.shape[0])\n",
    "    train_wanted.append(tdf[int(rand_index)])\n",
    "    train_labels_wanted.append(train_labels[int(rand_index)])\n",
    "\n",
    "train_wanted = np.asarray(train_wanted)\n",
    "train_labels_wanted = np.asarray(train_labels_wanted)\n",
    "\n",
    "train_equal = np.concatenate((train_numeric_pos, train_wanted), axis=0)\n",
    "train_labels_equal = np.concatenate((train_labels_pos, train_labels_wanted), axis=0)\n",
    "\n",
    "gbm = xgb.XGBClassifier(max_depth=30, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "gbm.fit(train_equal, train_labels_equal)\n",
    "\n",
    "print(metrics.accuracy_score(dev_labels, gbm.predict(ddf)))\n",
    "print(metrics.roc_auc_score(dev_labels, gbm.predict(ddf)))\n",
    "print(metrics.accuracy_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "print(metrics.roc_auc_score(train_labels_equal, gbm.predict(train_equal)))\n",
    "\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='roc_auc'))\n",
    "print(cross_validation.cross_val_score(gbm, train_equal, train_labels_equal, cv=10, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
